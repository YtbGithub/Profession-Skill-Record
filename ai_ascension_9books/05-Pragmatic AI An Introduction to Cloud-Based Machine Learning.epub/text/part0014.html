<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:ns="http://www.w3.org/2001/10/synthesis" xml:lang="en-us" lang="en-us">
  <head>
    <title>4 Cloud AI Development with Google Cloud Platform</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="../stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<h2 class="h1" id="ch04"><span epub:type="pagebreak" id="page_61" class="calibre2"></span>4</h2>
<h2 class="h2a">Cloud AI Development with Google Cloud Platform</h2>
<p class="blockquote"><em class="calibre5">There are no shortcuts to building a team each season. You build the foundation brick by brick.</em></p>
<p class="attribution">Bill Belichick</p>
<p class="noindent">For both developers and data scientists, there is a lot to like about the GCP. Many of their services are targeted toward making the development experience fun and powerful. In some ways, Google has been a leader in several aspects of the cloud that AWS has only recently started to address in a serious way. App Engine was way ahead of its time, released in 2008 as a Python-based Platform as a Service (PAAS) with access to Google Cloud Datastore, a fully managed NoSQL database service.</p>
<p class="noindent">In other ways, Amazon has created considerable moats in directly addressing the needs of the customer as their top priority. Amazon has customer obsession as one of its core values, as well as frugality. This combination has led to an onslaught of low-cost cloud services and features that Google at first seemed unwilling or unable to compete with. For years it was impossible to even find a phone number for any Google service, and even early innovations like Google App Engine were left to almost die on the vine through neglect. The real driver of revenue at Google has always been advertising, where at Amazon it has been products. The result has been for AWS to create a crushing lead of around 30 to 35 percent market share of the cloud worldwide.</p>
<p class="noindent">The explosion of interest in AI and “applied” Big Data has created opportunities for the Google Cloud, and it has pounced on them. Google has arguably been an ML and Big Data company from Day 1, which creates considerable weapons to attack the cloud market leader, AWS. While Google has been behind the curve on some of its cloud offerings, it has also been ahead on some its ML and AI offerings. A new battleground in cloud ML and AI services has emerged, and GCP is poised to be one of the strongest competitors in this arena.</p>
<h3 id="ch04lev1" class="calibre12"><span epub:type="pagebreak" id="page_62" class="calibre2"></span>GCP Overview</h3>
<p class="noindent">Google cleared about approximately 74 billion dollars in net digital ad sales, but only about 4 billion dollars from cloud services in 2017. The upside to this disparity is that there is plenty of money to subsidize research and development on new innovations in cloud services. A good example of this is TPUs, which are 15 to 30 times faster (<a href="https://cloud.google.com/blog/big-data/2017/05/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu" class="calibre7">https://cloud.google.com/blog/big-data/2017/05/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu</a>) than contemporary GPUs and CPUs.</p>
<p class="noindent">Beyond building dedicated chips for AI, Google has also been busy creating useful AI services with out-of-the-box pretrained models like the Cloud Vision API, Cloud Speech API, and the Cloud Translation API. One theme in this book is the concept of pragmatism. There is no shortage of companies in the San Francisco Bay Area stacked full of engineers and data scientists working on things that don’t matter. They busily toil away at Jupyter Notebooks that may never add real value to an organization.</p>
<p class="noindent">This is as real a problem as the front-end developer who keeps rewriting the web site from backbone to Angular to Vue.js on a predictable 6-month cadence. For the developer, yes, there is some value in doing this because it means they can update their resume with that framework. They are also hurting themselves because they are failing to learn how to create useful production solutions from what they have.</p>
<p class="noindent">Here is where pretrained models and high-level tools from GCP come into play. Many companies would benefit from calling these APIs versus doing the data science equivalent of porting their front-end to the Javascript framework of the month. This could also serve to add some buffer to teams in allowing them to deliver results quickly and simultaneously work on harder problems.</p>
<p class="noindent">Other high-level services that GCP provides can add a lot of value to a data science team. One of these services is Datalab, which has a similar “free” product associated with it called Colaboratory (<a href="https://colab.research.google.com/" class="calibre7">https://colab.research.google.com/</a>). These services create incredible value right out of the gate by eliminating package management hell. Additionally, because they have easy integration with the GCP platform, it becomes much easier to test out services and prototype solutions. Moreover, with the purchase of Kaggle (<a href="http://kaggle.com/" class="calibre7">http://kaggle.com/</a>), there is a much deeper integration with the Google Suite of tools, which may make it easier to hire data scientists who understand how to use BigQuery, for example. This was a smart move on the part of Google.</p>
<p class="noindent">One direction the GCP platform has taken that also distinguishes it from AWS is the exposure of high-level PaaS services, like Firebase (<a href="https://firebase.google.com/" class="calibre7">https://firebase.google.com/</a>).</p>
<h3 id="ch04lev2" class="calibre12">Colaboratory</h3>
<p class="noindent">Colaboratory is a research project out of Google that requires no setup and runs in the cloud (<a href="https://colab.research.google.com/" class="calibre7">https://colab.research.google.com/</a>). It is based on Jupyter Notebook, is free to use, and has a lot of packages preinstalled like Pandas, matplotlib, and TensorFlow. There are some extremely useful features right out of the box that make it interesting for many use cases.</p>
<p class="noindent">The features that I found the most interesting are as follows.</p>
<ul class="calibre13">
<li class="calibre14"><p class="bullet">Easy integration with Google Sheets, Google Cloud Storage, and your local filesystem with the ability to convert them to Pandas DataFrames</p></li>
<li class="calibre14"><p class="bullet"><span epub:type="pagebreak" id="page_63"></span>Support for both Python 2 and 3</p></li>
<li class="calibre14"><p class="bullet">Ability to upload notebooks</p></li>
<li class="calibre14"><p class="bullet">Notebooks are stored in Google Drive and can piggyback on the same sharing features that make Google Drive documents easy to work with</p></li>
<li class="calibre14"><p class="bullet">Ability for two users to edit the notebook at the same time</p></li>
</ul>
<p class="noindent">One of the more fascinating parts of Colaboratory is the ability to create a shared training lab for Jupyter Notebook–based projects. Several incredibly tricky problems are solved right away like sharing, dealing with data sets, and having libraries installed. An additional use case would be to programmatically create Colaboratory notebooks that have hooks into the AI pipeline of your organization. A directory of notebooks could be prepopulated with access to a BigQuery query or an ML model that is a template for a future project.</p>
<p class="noindent">Here is a hello-world workflow. You can refer to this notebook in GitHub: <a href="https://github.com/noahgift/pragmaticai-gcp/blob/master/notebooks/dataflow_sheets_to_pandas.ipynb" class="calibre7">https://github.com/noahgift/pragmaticai-gcp/blob/master/notebooks/dataflow_sheets_to_pandas.ipynb.</a> In <a href="part0014.html#ch4fig1" class="calibre7">Figure 4.1</a>, a new notebook is created.</p>
<div class="figure">
<div class="image"><a id="ch4fig1" class="calibre7"></a><img src="../images/00015.jpeg" aria-describedby="alt_04fig01" alt="A screenshot of a new notebook creation is shown." class="calibre8"/>
<aside class="hidden" id="alt_04fig01" data-AmznRemoved-M8="true" data-AmznRemoved="mobi7">
<p class="calibre21">The screen displays, Recent notebooks where a notebook, Hello-Colab-Data-Workflow is shown selected from the filter option. Three column headers: Title, First opened, and Last opened contains No results. A dialog box at the bottom right displays two options: NEW PYTHON 2 NOTEBOOK and NEW PYTHON 3 NOTEBOOK.</p>
</aside>
</div>
<p class="fig_caption"><span class="calibre6">Figure 4.1</span> Colaboratory Notebook Creation</p>
</div>
<p class="noindent">Next, the gspread library is installed.</p>
<p class="codelink"><a id="p063pro01" href="part0033_split_000.html#p063pro01a" class="calibre7">Click here to view code image</a></p>
<p class="pre">!pip install --upgrade -q gspread</p>
<p class="noindent"><span epub:type="pagebreak" id="page_64"></span>Authentication is necessary to write to a spreadsheet as shown, and a “gc” object is created as a result.</p>
<p class="codelink"><a id="p064pro01" href="part0033_split_001.html#p064pro01a" class="calibre7">Click here to view code image</a></p>
<p class="pre">from google.colab import auth<br class="calibre9"/>
auth.authenticate_user()<br class="calibre9"/>
<br class="calibre9"/>
import gspread<br class="calibre9"/>
from oauth2client.client import GoogleCredentials<br class="calibre9"/>
<br class="calibre9"/>
gc = gspread.authorize(GoogleCredentials.get_application_default())</p>
<p class="noindent">The gc object is now used to create a spreadsheet that has one column populated with values from 1 to 10.</p>
<p class="codelink"><a id="p064pro02" href="part0033_split_002.html#p064pro02a" class="calibre7">Click here to view code image</a></p>
<p class="pre">sh = gc.create('pramaticai-test')<br class="calibre9"/>
worksheet = gc.open('pramaticai-test').sheet1<br class="calibre9"/>
cell_list = worksheet.range('A1:A10')<br class="calibre9"/>
<br class="calibre9"/>
import random<br class="calibre9"/>
count = 0<br class="calibre9"/>
for cell in cell_list:<br class="calibre9"/>
  count +=1<br class="calibre9"/>
  cell.value = count<br class="calibre9"/>
worksheet.update_cells(cell_list)</p>
<p class="noindent">Finally, the spreadsheet is converted to a Pandas DataFrame:</p>
<p class="codelink"><a id="p064pro03" href="part0033_split_003.html#p064pro03a" class="calibre7">Click here to view code image</a></p>
<p class="pre">worksheet = gc.open('pramaticai-test').sheet1<br class="calibre9"/>
rows = worksheet.get_all_values()<br class="calibre9"/>
import pandas as pd<br class="calibre9"/>
df = pd.DataFrame.from_records(rows)</p>
<h3 id="ch04lev3" class="calibre12">Datalab</h3>
<p class="noindent">The next stop on the tour of GCP is Datalab (<a href="https://https://cloud.google.com/datalab/docs/quickstart" class="calibre7">cloud.google.com/datalab/docs/quickstart</a>). The entire gcloud ecosystem requires the software development kit (SDK) be installed from  <a href="https://cloud.google.com/sdk/downloads" class="calibre7">https://cloud.google.com/sdk/downloads</a>. Another option to install is to use the terminal as follows.</p>
<p class="codelink"><a id="p064pro04" href="part0033_split_004.html#p064pro04a" class="calibre7">Click here to view code image</a></p>
<p class="pre">curl https://sdk.cloud.google.com | bash<br class="calibre9"/>
exec -l $SHELL<br class="calibre9"/>
gcloud init<br class="calibre9"/>
gcloud components install datalab</p>
<p class="noindent">Once the gcloud environment is initialized, a Datalab instance can be launched. There are a few interesting things to note. Docker is an incredible technology for running isolated versions of Linux that will run the same on your laptop as it will in a data center, or some other laptop of a future collaborator.</p>
<h4 id="ch04lev3sub1" class="calibre16">Extending Datalab with Docker and Google Container Registry</h4>
<p class="noindent">You can run Datalab locally in Docker by referring to the getting-started guide here:  <a href="https://github.com/googledatalab/datalab/wiki/Getting-Started" class="calibre7">https://github.com/googledatalab/datalab/wiki/Getting-Started</a>. Just having a local, friendly <span epub:type="pagebreak" id="page_65"></span>version of Datalab you can run free of charge is useful, but what is even more powerful is the ability to extend this Datalab base image, store it in your own Google Container Registry, then launch it with a more powerful instance than your workstation or laptop, like an n1-highmem-32, for example, which has 16 cores and 104GB of memory.</p>
<p class="noindent">All of a sudden, problems that were just not addressable from your local laptop become pretty straightforward. The workflow to extend the Datalab Docker core image is covered in the guide mentioned in this section. The main takeaway is that after cloning the repository, a change will need to be made to Dockerfile.in.</p>
<h4 id="ch04lev3sub2" class="calibre16">Launching Powerful Machines with Datalab</h4>
<p class="noindent">To launch one of these mega instances of Jupyter Notebook, it looks like this, as seen in <a href="part0014.html#ch4fig2" class="calibre7">Figure 4.2</a>.</p>
<p class="codelink"><a id="p065pro01" href="part0033_split_005.html#p065pro01a" class="calibre7">Click here to view code image</a></p>
<p class="pre">→  pragmaticai-gcp git:(master) datalab create\<br class="calibre9"/>
 --machine-type n1-highmem-16 pragai-big-instance<br class="calibre9"/>
Creating the instance pragai-big-instance<br class="calibre9"/>
Created [https://www.googleapis.com/compute/v1<br class="calibre9"/>
/projects/cloudai-194723/zones/us-central1-f/<br class="calibre9"/>
instances/pragai-big-instance].<br class="calibre9"/>
Connecting to pragai-big-instance.<br class="calibre9"/>
This will create an SSH tunnel and may prompt you<br class="calibre9"/>
to create an rsa key pair. To manage these keys, see<br class="calibre9"/>
https://cloud.google.com/compute/docs/instances/\<br class="calibre9"/>
adding-removing-ssh-keys<br class="calibre9"/>
Waiting for Datalab to be reachable at http://localhost:8081/<br class="calibre9"/>
Updating project ssh metadata...-    </p>
<div class="figure">
<div class="image"><a id="ch4fig2" class="calibre7"></a><img src="../images/00016.jpeg" aria-describedby="alt_04fig02" alt="A screenshot displays the Datalab Instance Running in G C P Console." class="calibre8"/>
<aside class="hidden" id="alt_04fig02" data-AmznRemoved-M8="true" data-AmznRemoved="mobi7">
<p class="calibre21">The screen, VM instances displays the following options at the top: CREATE INSTANCE, INPORT VM, REFRESH, a play button, a stop button, a power button, a delete button, and SHOW INFO PANEL. A filter for VM instances is shown below and a drop-down list box to its right set to “Columns.” The results are listed below in table form. The column header reads: Name, Zone, Creation time, Machine type, Recommendation, Internal IP, External IP, and Connect. One of the results listed below is shown selected.</p>
</aside>
</div>
<p class="fig_caption"><span class="calibre6">Figure 4.2</span> Datalab Instance Running in GCP Console</p>
</div>
<p class="noindent">To make the instance do something useful, Major League Baseball game logs from 1871 to 2016 that were found at data.world (<a href="https://data.world/dataquest/mlb-game-logs" class="calibre7">https://data.world/dataquest/mlb-game-logs</a>) are synced to a bucket in GCP. <a href="part0014.html#ch4fig3" class="calibre7">Figure 4.3</a> shows that 171,000 rows were loaded into Pandas DataFrame via the <code class="calibre11">describe</code> command.</p>
<p class="codelink"><a id="p065pro02" href="part0033_split_006.html#p065pro02a" class="calibre7">Click here to view code image</a></p>
<p class="pre">gsutil cp game_logs.csv gs://pragai-datalab-test<br class="calibre9"/>
Copying file://game_logs.csv [Content-Type=text/csv]...<br class="calibre9"/>
- [1 files][129.8 MiB/129.8 MiB]<br class="calibre9"/>
 628.9 KiB/s                                  <br class="calibre9"/>
Operation completed over 1 objects/129.8 MiB.      </p>
<div class="figure">
<div class="image"><span epub:type="pagebreak" id="page_66"></span><a id="ch4fig3" class="calibre7"></a><img src="../images/00017.jpeg" aria-describedby="alt_04fig03" alt="A screenshot displays 171,000 rows in Pandas DataFrame from G C P Bucket." class="calibre8"/>
<aside class="hidden" id="alt_04fig03" data-AmznRemoved-M8="true" data-AmznRemoved="mobi7">
<p class="calibre21">The screen displays two fields: In [19] with a text box beside reads, df.describe ( ) and Out [19] which shows a table with 8 rows and 8 columns. The column header reads: data, number_of_game, v_game_number, h_game_number, v_score, h_score, length_outs, and attendance. The row header reads: count, mean, std, min, 25 percent, 50 percent, 75 percent, and max. The table consists of the loaded data.</p>
</aside>
</div>
<p class="fig_caption"><span class="calibre6">Figure 4.3</span> 171,000 Rows in DataFrame from GCP Bucket</p>
</div>
<p class="noindent">The entire notebook can be found in GitHub (<a href="https://github.com/noahgift/pragmaticai-gcp/blob/master/notebooks/pragai-big-instance.ipynb" class="calibre7">https://github.com/noahgift/pragmaticai-gcp/blob/master/notebooks/pragai-big-instance.ipynb</a>), and the commands are as follows. First, some imports.</p>
<p class="codelink"><a id="p066pro01" href="part0033_split_007.html#p066pro01a" class="calibre7">Click here to view code image</a></p>
<p class="pre">Import pandas as pd<br class="calibre9"/>
pd.set_option('display.float_format', lambda x: '%.3f' % x)<br class="calibre9"/>
import seaborn as sns<br class="calibre9"/>
from io import BytesIO</p>
<p class="noindent">Next, a special Datalab command assigns the output to a variable called <code class="calibre11">game_logs.</code></p>
<p class="codelink"><a id="p066pro02" href="part0033_split_008.html#p066pro02a" class="calibre7">Click here to view code image</a></p>
<p class="pre">%gcs read --object gs://pragai-datalab-test/game_logs.csv\<br class="calibre9"/>
          --variable game_logs</p>
<p class="noindent">A new DataFrame is now created.</p>
<p class="codelink"><a id="p066pro03" href="part0033_split_009.html#p066pro03a" class="calibre7">Click here to view code image</a></p>
<p class="pre">df = pd.read_csv(BytesIO(game_logs))</p>
<p class="noindent">Finally, the DataFrame is plotted as shown in <a href="part0014.html#ch4fig4" class="calibre7">Figure 4.4</a>.</p>
<p class="codelink"><a id="p066pro04" href="part0033_split_010.html#p066pro04a" class="calibre7">Click here to view code image</a></p>
<p class="pre">%timeit<br class="calibre9"/>
ax = sns.regplot(x="v_score", y="h_score", data=df)</p>
<div class="figure">
<div class="image"><span epub:type="pagebreak" id="page_67"></span><a id="ch4fig4" class="calibre7"></a><img src="../images/00018.jpeg" aria-describedby="alt_04fig04" alt="A screenshot displays the plotted DataFrame." class="calibre8"/>
<aside class="hidden" id="alt_04fig04" data-AmznRemoved-M8="true" data-AmznRemoved="mobi7">
<p class="calibre21">The screen displays the field, In [23] that reads the following command: %timeit ax equals sns.regplot (x=”v_score”, y =”h_score”, data=df) 14.7 s (plus or minus) 1.03 s per loop (mean (plus or minus) std. dev. Of 7 runs, 1 loop each) /usr/local/evens/py3env/lib/python3.5/site-packages/matplotlib/front_manager. Py:1320: UserWarning: indfont: Font family [ ‘sans-serif’ ] not found. Falling back to DejaVu sans (prop.get_family(), self.defaultFamily[fontext])). A plotted graph is shown below. The horizontal axis of the graph represents v_score ranging from 0 to 50, in increments of 10 and the vertical axis represents h_score ranging from 0 to 40, in increments of 5.</p>
</aside>
</div>
<p class="fig_caption"><span class="calibre6">Figure 4.4</span> Timed Seaborn Plot Takes 17 seconds to Plot 171,000 Rows on 32 Core Machine with 100GB of Memory</p>
</div>
<p class="noindent">The takeaway from this exercise is that using Datalab to spin up ridiculously powerful machines to do exploratory data analysis (EDA) is a good idea. Billing is by the second, and it can save hours to do heavy crunching and EDA. Another takeaway is that GCP is leading this section of cloud services versus AWS because the developer experience does a quite nice job in moving large sets of data into a notebook and using familiar “small data” tools like Seaborn and Pandas.</p>
<p class="noindent">Another takeaway is that Datalab is a great foundation for building production ML pipelines. GCP buckets can be explored, BigQuery has straightforward integration, and other parts of the GCP ecosystem can be integrated like ML Engine, TPUs, and the container registry.</p>
<h3 id="ch04lev4" class="calibre12">BigQuery</h3>
<p class="noindent">BigQuery is one of crown jewels of the GCP ecosystem and an excellent service around which to build pragmatic production ML and AI pipelines. In many ways, this too has some developer usability advantages over AWS. While AWS is stronger on complete end-to-end solutions, GCP seems to cater to the tools and paradigms developers are already used to. It is trivial to move data in and out in a variety of different ways, from the command line on a local machine to GCP buckets to API calls.</p>
<h4 id="ch04lev4sub1" class="calibre16">Moving Data into BigQuery from the Command Line</h4>
<p class="noindent">Speaking of moving data into BigQuery, a straightforward way to do this is to use the <code class="calibre11">bq</code> command-line tool. Here is the recommended approach.</p>
<p class="noindent">First, check whether the default project has any existing data sets.</p>
<p class="codelink"><a id="p067pro04" href="part0033_split_011.html#p067pro04a" class="calibre7">Click here to view code image</a></p>
<p class="pre">→  pragmaticai-gcp git:(master) bq ls</p>
<p class="noindent">In this case, there isn’t an existing data set in the default project, so a new data set will be created.</p>
<p class="codelink"><a id="p067pro02" href="part0033_split_012.html#p067pro02a" class="calibre7">Click here to view code image</a></p>
<p class="pre">→  pragmaticai-gcp git:(master) bq mk gamelogs<br class="calibre9"/>
Dataset 'cloudai:gamelogs' successfully created.</p>
<p class="noindent">Next, <code class="calibre11">bq ls</code> is used to verify the data set was created.</p>
<p class="codelink"><a id="p067pro03" href="part0033_split_013.html#p067pro03a" class="calibre7">Click here to view code image</a></p>
<p class="pre">→  pragmaticai-gcp git:(master) bq ls        <br class="calibre9"/>
  datasetId  <br class="calibre9"/>
 -----------<br class="calibre9"/>
  gamelogs</p>
<p class="noindent"><span epub:type="pagebreak" id="page_68"></span>Next, a local CSV file that is 134MB and has 171,000 records is uploaded with the flag - <code class="calibre11">autodetect.</code> This flag enables a lazier way to upload data sets with many columns since you don’t have to define the schema up front.</p>
<p class="codelink"><a id="p068pro01" href="part0033_split_014.html#p068pro01a" class="calibre7">Click here to view code image</a></p>
<p class="pre">→  pragmaticai-gcp git:(master) ✗ bq load\<br class="calibre9"/>
 --autodetect gamelogs.records game_logs.csv<br class="calibre9"/>
Upload complete.<br class="calibre9"/>
Waiting on bqjob_r3f28bca3b4c7599e_00000161daddc035_1<br class="calibre9"/>
        ... (86s) Current status: DONE  </p>
<p class="pre">→  pragmaticai-gcp git:(master) ✗ du -sh game_logs.csv<br class="calibre9"/>
134M    game_logs.csv</p>
<p class="noindent">Now that the data has been loaded, it can be queried easily from Datalab as shown in <a href="part0014.html#ch4fig5" class="calibre7">Figure 4.5</a>.</p>
<div class="figure">
<div class="image"><a id="ch4fig5" class="calibre7"></a><img src="../images/00019.jpeg" aria-describedby="alt_04fig05" alt="A screenshot of BigQuery to Pandas to Seaborn Pipeline is shown." class="calibre8"/>
<aside class="hidden" id="alt_04fig05" data-AmznRemoved-M8="true" data-AmznRemoved="mobi7">
<p class="calibre21">The screen displays the command df.head() and a table consists of 5 rows and 161 columns are shown. The following commands below reads: g = sns. FacetGrid(df, col- ‘day_of_week”, size=4, aspect=.5 g = g.map(plt.hist, “attendance”) /usr/local/evens/py3env/lib/python3.5/site-packages/matplotlib/front_manager. Py:1320: UserWarning: indfont: Font family [ ‘sans-serif’ ] not found. Falling back to DejaVu sans (prop.get_family(), self.defaultFamily[fontext])). Seven vertical bar graphs that represents day_of _week = Sat, day_of _week = Thu, day_of _week = Tue, day_of _week = Fri, day_of _week = Mon, day_of _week = Wed, and day_of _week = Sun. The horizontal axis of the graphs represents “attendance” ranging from 0 to 30000, in increments of 10000 and the vertical axis ranging from 0 to 50, in increments of 10.</p>
</aside>
</div>
<p class="fig_caption"><span class="calibre6">Figure 4.5</span> BigQuery to Pandas to Seaborn Pipeline</p>
</div>
<p class="noindent">First, imports are created. Note the google.datalab.bigquery import, which allows for easy access to BigQuery.</p>
<p class="codelink"><a id="p068pro02" href="part0033_split_015.html#p068pro02a" class="calibre7">Click here to view code image</a></p>
<p class="pre">import pandas as pd<br class="calibre9"/>
import google.datalab.bigquery as bq<br class="calibre9"/>
pd.set_option('display.float_format', lambda x: '%.3f' % x)<br class="calibre9"/>
import seaborn as sns<br class="calibre9"/>
from io import BytesIO</p>
<p class="noindent"><span epub:type="pagebreak" id="page_69"></span>Next, a query is turned into a DataFrame. There are 171,000 rows, but for this query, it is limited to 10 rows.</p>
<p class="codelink"><a id="p069pro01" href="part0033_split_016.html#p069pro01a" class="calibre7">Click here to view code image</a></p>
<p class="pre">some_games = bq.Query('SELECT * FROM `gamelogs.records` LIMIT 10000')<br class="calibre9"/>
df = some_games.execute(output_options=\<br class="calibre9"/>
        bq.QueryOutput.dataframe()).result()</p>
<p class="noindent">Finally, that DataFrame is converted into a Seaborn visualization that facets the plots by day of the week.</p>
<p class="codelink"><a id="p069pro02" href="part0033_split_017.html#p069pro02a" class="calibre7">Click here to view code image</a></p>
<p class="pre">g = sns.FacetGrid(df, col="day_of_week", size=4, aspect=.5)<br class="calibre9"/>
g = g.map(plt.hist, "attendance")</p>
<p class="noindent">The takeaway from this pipeline example is that BigQuery and Datalab are very impressive and facilitate the creation of easy ML pipelines. Within minutes, it is possible to upload gigantic data sets to BigQuery, fire up a “God” Jupyter workstation to do EDA, and then convert it into actionable notebooks.</p>
<p class="noindent">This toolchain could next plug right into the ML services of Google or plug into training custom classification models using TPUs. As mentioned before, one of the more powerful differences from the AWS platform is the workflow that encourages the use of the common open-source libraries like Seaborn and Pandas. On one hand, it is true that these tools will eventually fall over under large enough data sets; however, it does add a layer of convenience to be able to reach for familiar tools.</p>
<h3 id="ch04lev5" class="calibre12">Google Cloud AI Services</h3>
<p class="noindent">As mentioned in the title of this book, pragmatism in AI is heavily encouraged. Why not use off-the-shelf tools when they are available? Fortunately, GCP includes quite a few services, ranging from wild experiments to legitimate “build your company around them” services. Here is brief list of some of the highlights.</p>
<ul class="calibre13">
<li class="calibre14"><p class="bullet">Cloud AutoML (<a href="https://cloud.google.com/automl/" class="calibre7">https://cloud.google.com/automl/</a>)</p></li>
<li class="calibre14"><p class="bullet">Cloud TPU (<a href="https://cloud.google.com/tpu/" class="calibre7">https://cloud.google.com/tpu/</a>)</p></li>
<li class="calibre14"><p class="bullet">Cloud Machine Learning Engine (<a href="https://cloud.google.com/ml-engine/" class="calibre7">https://cloud.google.com/ml-engine/</a>)</p></li>
<li class="calibre14"><p class="bullet">Cloud Job Discovery (<a href="https://cloud.google.com/job-discovery/" class="calibre7">https://cloud.google.com/job-discovery/</a>)</p></li>
<li class="calibre14"><p class="bullet">Cloud Dialogflow Enterprise Edition (<a href="https://cloud.google.com/dialogflow-enterprise/" class="calibre7">https://cloud.google.com/dialogflow-enterprise/</a>)</p></li>
<li class="calibre14"><p class="bullet">Cloud Natural Language (<a href="https://cloud.google.com/natural-language/" class="calibre7">https://cloud.google.com/natural-language/</a>)</p></li>
<li class="calibre14"><p class="bullet">Cloud Speech-to API (<a href="https://cloud.google.com/speech/" class="calibre7">https://cloud.google.com/speech/</a>)</p></li>
<li class="calibre14"><p class="bullet">Cloud Translation API (<a href="https://cloud.google.com/translate/" class="calibre7">https://cloud.google.com/translate/</a>)</p></li>
<li class="calibre14"><p class="bullet">Cloud Vision API (<a href="https://cloud.google.com/vision/" class="calibre7">https://cloud.google.com/vision/</a>)</p></li>
<li class="calibre14"><p class="bullet">Cloud Video Intelligence (<a href="https://cloud.google.com/video-intelligence/" class="calibre7">https://cloud.google.com/video-intelligence/</a>)</p></li>
</ul>
<p class="noindent"><span epub:type="pagebreak" id="page_70"></span>Another use case for the AI services is to supplement your existing data center or cloud. Why not try the Cloud Natural Language service on your AWS data sets and compare how both work instead of, God forbid, training your own natural language model? Pragmatic AI teams are wise to pick these off-the-shelf solutions, put them into production, and focus on doing bespoke ML training for areas where it really matters to be different.</p>
<p class="noindent">To use these services, the recommended workflow would be much like other examples in this chapter: fire up a Datalab instance, upload some data, and play around with the API. Another way to explore the API, though, is by just uploading data in the API explorer for each of these services. In many situations, this is the ideal way to start.</p>
<h4 id="ch04lev5sub1" class="calibre16">Classifying my Crossbreed Dog with the Google Vision API</h4>
<p class="noindent">To use the Computer Vision API via the API explorer, <a href="https://cloud.google.com/vision/docs/quickstart" class="calibre7">https://cloud.google.com/vision/docs/quickstart</a> contains a quickstart example. To test it out, I uploaded a picture of my dog into a bucket <code class="calibre11">pragai-cloud-vision</code> and called it <code class="calibre11">titan_small.jpg</code>.</p>
<p class="noindent">Next, in <a href="part0014.html#ch4fig6" class="calibre7">Figure 4.6</a> an API call is formulated to that bucket/file. In <a href="part0014.html#ch4fig7" class="calibre7">Figure 4.7</a>, it’s Titan the  family dog.</p>
<div class="figure">
<div class="image"><a id="ch4fig6" class="calibre7"></a><img src="../images/00020.jpeg" aria-describedby="alt_04fig06" alt="A screenshot displays Google Vision API Request in Browser." class="calibre8"/>
<aside class="hidden" id="alt_04fig06" data-AmznRemoved-M8="true" data-AmznRemoved="mobi7">
<p class="calibre21">The screen displays, Request body section that reads the following coding: { “requests”: [ { “features”: [ { “type”: “LABEL_DRTECTION” (a hint bubble) } (a hint bubble) ], “image”: { “source”: { “imageUri”: “ gs://pragai-cloud-vision/titan_small.jpg” (a hint bubble) } (a hint bubble) } (a hint bubble) } (a hint bubble) ] } At the bottom of the Request body an instruction reads, Press control plus space or click on the hint bubbles for suggestions.</p>
</aside>
</div>
<p class="fig_caption"><span class="calibre6">Figure 4.6</span> Google Vision API Request in Browser</p>
</div>
<div class="figure">
<p class="image2"><span epub:type="pagebreak" id="page_71"></span><a id="ch4fig7" class="calibre7"></a><img src="../images/00021.jpeg" alt="A photograph of a crossbreed dog is shown." class="calibre8"/></p>
<p class="fig_caption"><span class="calibre6">Figure 4.7</span> Titan’s Cute Stare Didn’t Fool Google’s AI</p>
</div>
<p class="noindent">So, what did the image classification system discover about the family dog? It turns out our dog has a greater then 50% chance of being a Dalmatian. It also looks like our dog is a crossbreed.</p>
<p class="codelink"><a id="p070pro01" href="part0033_split_018.html#p070pro01a" class="calibre7">Click here to view code image</a></p>
<p class="pre">{<br class="calibre9"/>
  "responses": [<br class="calibre9"/>
    {<br class="calibre9"/>
      "labelAnnotations": [<br class="calibre9"/>
        {<br class="calibre9"/>
          "mid": "/m/0bt9lr",<br class="calibre9"/>
          "description": "dog",<br class="calibre9"/>
          "score": 0.94724846,<br class="calibre9"/>
          "topicality": 0.94724846<br class="calibre9"/>
        },<br class="calibre9"/>
        {<br class="calibre9"/>
          "mid": "/m/0kpmf",<br class="calibre9"/>
          "description": "dog breed",<br class="calibre9"/>
          "score": 0.91325045,<br class="calibre9"/>
          "topicality": 0.91325045<br class="calibre9"/>
        },<br class="calibre9"/>
        {<br class="calibre9"/>
          "mid": "/m/05mqq3",<br class="calibre9"/>
          "description": "snout",<br class="calibre9"/>
          "score": 0.75345945,<br class="calibre9"/>
          "topicality": 0.75345945<br class="calibre9"/>
        },<br class="calibre9"/>
        {<br class="calibre9"/>
          "mid": "/m/01z5f",<br class="calibre9"/>
          "description": "dog like mammal",<br class="calibre9"/>
          "score": 0.7018985,<br class="calibre9"/>
          "topicality": 0.7018985<br class="calibre9"/>
        },<br class="calibre9"/>
        {<br class="calibre9"/>
          "mid": "/m/02rjc05",<br class="calibre9"/>
          "description": "dalmatian",<br class="calibre9"/>
          "score": 0.6340561,<br class="calibre9"/>
          "topicality": 0.6340561<br class="calibre9"/>
        },<br class="calibre9"/>
        {<br class="calibre9"/>
          "mid": "/m/02xl47d",<br class="calibre9"/>
          "description": "dog breed group",<br class="calibre9"/>
          "score": 0.6023531,<br class="calibre9"/>
          "topicality": 0.6023531<br class="calibre9"/>
        },<br class="calibre9"/>
        {<br class="calibre9"/>
          "mid": "/m/03f5jh",<br class="calibre9"/>
          "description": "dog crossbreeds",<br class="calibre9"/>
          "score": 0.51500386,<br class="calibre9"/>
          "topicality": 0.51500386<br class="calibre9"/>
        }<br class="calibre9"/>
      ]<br class="calibre9"/>
    }<br class="calibre9"/>
  ]<br class="calibre9"/>
}</p>
<h3 id="ch04lev6" class="calibre12"><span epub:type="pagebreak" id="page_72" class="calibre2"></span>Cloud TPU and TensorFlow</h3>
<p class="noindent">An emerging trend in 2018 is the appearance of custom ML accelerators. In February 2018, Google put TPUs into beta release, but they have been in use internally at Google on products such as Google Image Search, Google Photos, and Google Cloud Vision API. A more technical read of TPUs can be found by reading In-Datacenter Performance of a Tensor Processing Unit (<a href="https://drive.google.com/file/d/0Bx4hafXDDq2EMzRNcy1vSUxtcEk/view" class="calibre7">https://drive.google.com/file/d/0Bx4hafXDDq2EMzRNcy1vSUxtcEk/view</a>). This mentions the “Cornucopia Corollary” to Amdah’s Law: <em class="calibre5">Low utilization of a huge cheap resource can still deliver high, cost-effective performance</em>.</p>
<p class="noindent"><span epub:type="pagebreak" id="page_73"></span>TPUs, like some of the other AI services coming out of Google, have the ability to be game-changing technological moats. TPU, in particular, is an interesting bet. If Google can make it easy to train deep learning models with the TensorFlow SDK and enable significant efficiencies by utilizing dedicated hardware AI accelerators, it could have a massive advantage over other clouds.</p>
<p class="noindent">Ironically though, although Google has made incredible strides at making parts of its cloud ecosystem ridiculously developer friendly, the TensorFlow SDK continues to be problematic. It is very low level, complex, and seemingly designed for Math PhDs who prefer to write in Assembly and C++. There are some solutions that help mitigate this though, like PyTorch. As much as I am personally excited about TPUs, I do think there needs to be a “come to Jesus” moment on the complexity of TensorFlow. It probably will happen, but it is worth noting, HBD… “Here Be Dragons.”</p>
<h4 id="ch04lev6sub1" class="calibre16">Running MNIST on Cloud TPUs</h4>
<p class="noindent">This tutorial is going to piggyback off an existing tutorial for the TPUs that are in beta at the time of this book’s publishing. That tutorial can be found at <a href="https://cloud.google.com/tpu/docs/tutorials/mnist" class="calibre7">https://cloud.google.com/tpu/docs/tutorials/mnist</a>. To get started, not only is it necessary to have the gcloud SDK installed, but the beta components are needed as well.</p>
<p class="codelink"><a id="p073pro01" href="part0033_split_020.html#p073pro01a" class="calibre7">Click here to view code image</a></p>
<p class="pre">gcloud components install beta</p>
<p class="noindent">Next, a virtual machine (VM) is needed that will be the job controller. The gcloud cli is used to create a 4-core VM in the central region.</p>
<p class="codelink"><a id="p073pro02" href="part0033_split_021.html#p073pro02a" class="calibre7">Click here to view code image</a></p>
<p class="pre">(.tpu) →  google-cloud-sdk/bin/gcloud compute instances\<br class="calibre9"/>
    create tpu-demo-vm \<br class="calibre9"/>
  --machine-type=n1-standard-4 \<br class="calibre9"/>
  --image-project=ml-images \<br class="calibre9"/>
  --image-family=tf-1-6 \<br class="calibre9"/>
  --scopes=cloud-platform<br class="calibre9"/>
Created [https://www.googleapis.com/compute/v1/\<br class="calibre9"/>
        projects/cloudai-194723/zones/us-central1-f/\<br class="calibre9"/>
        instances/tpu-demo-vm].<br class="calibre9"/>
NAME         ZONE           MACHINE_TYPE  <br class="calibre9"/>
STATUS<br class="calibre9"/>
tpu-demo-vm  us-central1-f  n1-standard-4  _            <br class="calibre9"/>
RUNNING</p>
<p class="noindent">After the VM is created, a TPU instance needs to be provisioned.</p>
<p class="codelink"><a id="p073pro03" href="part0033_split_022.html#p073pro03a" class="calibre7">Click here to view code image</a></p>
<p class="pre">google-cloud-sdk/bin/gcloud beta compute tpus create demo-tpu \<br class="calibre9"/>
  --range=10.240.1.0/29 --version=1.6<br class="calibre9"/>
Waiting for [projects/cloudai-194723/locations/us-central1-f/\<br class="calibre9"/>
operations/operation-1518469664565-5650a44f569ac-9495efa7-903<br class="calibre9"/>
9887d] to finish...done.<br class="calibre9"/>
Created [demo-tpu].<br class="calibre9"/>
<br class="calibre9"/>
google-cloud-sdk/bin/gcloud compute ssh tpu-demo-vm -- -L \<br class="calibre9"/>
        6006:localhost:6006</p>
<p class="noindent"><span epub:type="pagebreak" id="page_74"></span>Next, some data for the project is downloaded and then uploaded to Cloud Storage. Note that my bucket is called <code class="calibre11">tpu-research</code>, but your bucket will be different.</p>
<p class="codelink"><a id="p074pro01" href="part0033_split_023.html#p074pro01a" class="calibre7">Click here to view code image</a></p>
<p class="pre">python /usr/share/tensorflow/tensorflow/examples/how_tos/\<br class="calibre9"/>
        reading_data/convert_to_records.py --directory=./data<br class="calibre9"/>
gunzip ./data/*.gz<br class="calibre9"/>
export GCS_BUCKET=gs://tpu-research<br class="calibre9"/>
gsutil cp -r ./data ${STORAGE_BUCKET}</p>
<p class="noindent">Finally, the <code class="calibre11">TPU_NAME</code> variable needs to match the name of the TPU instance created earlier.</p>
<p class="pre">export TPU_NAME='demo-tpu'</p>
<p class="noindent">The last step is to train the model. In this example, the iterations are pretty low, and since the TPUs are powerful, it may make sense to experiment with adding a few zeros to the iterations.</p>
<p class="codelink"><a id="p074pro02" href="part0033_split_024.html#p074pro02a" class="calibre7">Click here to view code image</a></p>
<p class="pre">python /usr/share/models/official/mnist/mnist_tpu.py \<br class="calibre9"/>
  --tpu_name=$TPU_NAME \<br class="calibre9"/>
  --data_dir=${STORAGE_BUCKET}/data \<br class="calibre9"/>
  --model_dir=${STORAGE_BUCKET}/output \<br class="calibre9"/>
  --use_tpu=True \<br class="calibre9"/>
  --iterations=500 \<br class="calibre9"/>
  --train_steps=1000</p>
<p class="noindent">The model will then print out the loss value, and it is also possible to look at the tensorboard, which has many interesting graphical features. However, a final bit of cleanup will need to be done. Cleaning up the TPUs is necessary to eliminate further charges, as shown.</p>
<p class="codelink"><a id="p074pro03" href="part0033_split_025.html#p074pro03a" class="calibre7">Click here to view code image</a></p>
<p class="pre">noahgift@tpu-demo-vm:~$ gcloud beta compute tpus delete demo-tpu<br class="calibre9"/>
Your TPU [demo-tpu] will be deleted.<br class="calibre9"/>
Do you want to continue (Y/n)?  y<br class="calibre9"/>
Waiting for [projects/cloudai-194723/locations/us-central1-f/\<br class="calibre9"/>
operations/operation-1519805921265-566416410875e-018b840b<br class="calibre9"/>
-1fd71d53] to finish...done.                    <br class="calibre9"/>
Deleted [demo-tpu].</p>
<h3 id="ch04lev7" class="calibre12">Summary</h3>
<p class="noindent">GCP is a legitimate contender for building pragmatic AI solutions. It has some advantages and unique offerings over AWS, mainly around the hyper-focus on the developer experience and the off-the-shelf, high-level AI services.</p>
<p class="noindent">Recommended next steps for the curious AI practitioner would be to go through some of the AI APIs and see how they can be wired together to create solutions that “just work.” One of the opportunities and challenges that Google has created is the TPU and TensorFlow ecosystem. On one hand, it is extremely complex to get started with; on the other hand, the power is seductive. Being an expert at TPUs, at the very least, seems like a wise course of action for any company trying to be a leader in the AI space.</p>
</body></html>
