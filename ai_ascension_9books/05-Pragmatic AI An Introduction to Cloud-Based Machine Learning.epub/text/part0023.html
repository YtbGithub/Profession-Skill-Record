<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:ns="http://www.w3.org/2001/10/synthesis" xml:lang="en-us" lang="en-us">
  <head>
    <title>A AI Accelerators</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="../stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<h2 class="h1" id="appendixa"><span epub:type="pagebreak" id="page_217" class="calibre2"></span>A</h2>
<h2 class="h2a">AI Accelerators</h2>
<p class="noindent">AI accelerators are a relatively new but rapidly developing technology. There are few different categories: new or bespoke products, GPU-based products, AI co-processors, and R/D products. In the new-products category, TPU has perhaps the most buzz because it allows TensorFlow developers an easy pathway to developing software.</p>
<p class="noindent">GPU-based products are currently the most common form of AI acceleration. Professor Ian Lane, Carnegie Mellon University, says this about them: “With GPUs, pre-recorded speech or multimedia content can be transcribed much more quickly. Compared to CPU implementation we are able to perform recognition up to 33× faster.”</p>
<p class="noindent">In the category of FPGA, one company to look out for is reconfigure.io (<a href="https://reconfigure.io/" class="calibre7">https://reconfigure.io/</a>). Reconfigure.io enables developers to easily use FPGAs to accelerate their solutions, including for AI. Using simple tools, and a powerful cloud build-and-deploy capability, Reconfigure.io gives developers speed, latency, and power previously only available to hardware designers. They provide a Go-based interface that takes Go code, compiles and optimizes it, and deploys it to cloud-based FPGAs on AWS. FPGAs are especially useful for using AI in network situations and where low power is needed, and so they are being heavily used by major cloud providers.</p>
<p class="noindent">Although both GPUs and FPGAs do offer great performance improvements over CPUs, application-specific integrated circuits (ASICs) like TPUs can best these by a factor of 10. So the main use case in something like FPGAs is the familiarity that comes with being able to use tools like Go to more rapidly develop applications.</p>
<p class="noindentb">Some issues to consider with AI accelerators include the following.</p>
<ol class="calibre27">
<li value="1" class="calibre28"><p class="calibre21">What are the application performance requirements and general data center cost structure criteria when deciding whether to implement accelerators for use in an inference application?</p></li>
<li value="2" class="calibre28"><p class="calibre21">What are the use cases of a data center inference application being accelerated?</p></li>
</ol>
<p class="noindent">AI accelerators should be on the radar of every company looking to do cutting-edge AI. The reason is the performance. With GPUs and FPGAs getting 30× improvement over CPUs, and then dedicated ASICs getting another 10× improvement on top of that, it is too big a breakthrough to ignore. It could lead the way to developing forms of AI that we haven’t fully imagined.<span epub:type="pagebreak" id="page_218"></span></p>
</body></html>
