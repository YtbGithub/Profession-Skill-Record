<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:ns="http://www.w3.org/2001/10/synthesis" xml:lang="en-us" lang="en-us">
  <head>
    <title>5 Cloud AI Development with Amazon Web Services</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="../stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<h2 class="h1" id="ch05"><span epub:type="pagebreak" id="page_75" class="calibre2"></span>5</h2>
<h2 class="h2a">Cloud AI Development with Amazon Web Services</h2>
<p class="blockquote"><em class="calibre5">Your love makes me strong. Your hate makes me unstoppable</em>.</p>
<p class="attribution">Ronaldo</p>
<p class="noindent">FANG stocks (i.e., Facebook, Amazon, Netflix, and Google) have been growing at an unchecked pace in the last several years. Amazon alone in the last three years, from March 2015 to March 2018, has risen 300 percent. Netflix runs its operations on top of AWS as well. From a career perspective, there is a lot of momentum and money going into the AWS cloud. Understanding the platform and what it provides is crucial to the success of many AI applications in the  coming years.</p>
<p class="noindent">A takeaway from this large shift in capital is that the cloud is not only here to stay but is changing fundamental paradigms for the development of software. In particular, AWS has made a bet on serverless technology. The crown jewel of this stack is the Lambda technology, which allows functions in multiple languages—Go, Python, Java, C#, and Node—to be executed as events inside of a larger ecosystem. One way to think about this is that the cloud itself is a new  operating system.</p>
<p class="noindent">Python is famous for having some severe limitations for scalability because of the nature of the language. Despite the inspired, wrong, and <em class="calibre5">almost convincing</em>, arguments that the global interpreter lock (GIL) doesn’t matter and performance of Python doesn’t matter, it does in the real world at scale. What makes Python easy to use has historically also cursed it in terms of performance. In particular, the GIL effectively puts the brakes on efficient parallelization compared to other languages like Java at scale. Sure, there are workarounds on a Linux target host, but this often results in a lot of wasted engineering time rewriting the Erlang language concurrency ideas within Python poorly, or it results in idle cores.</p>
<p class="noindent"><span epub:type="pagebreak" id="page_76"></span>With AWS Lambda, that weakness becomes irrelevant because the operating system is AWS itself. Instead of using threads or processes to parallelize code, a cloud developer can use SNS, SQS, Lambda, and other building-block technologies. These primitives then take the place of threads, processes, and other traditional operating system paradigms. Further proof of the problems with scaling traditional Python on Linux center around deep investigation about supposed high scaling Python projects.</p>
<p class="noindent">If you dig deep, you will find that what actually is doing the hard work is something like RabbitMQ and/or Celery (written in Erlang), or Nginx, written in highly optimized C. But before you get too excited about Erlang (I ran a company that extensively used it), it is almost impossible to hire anyone who can write in it. The Go language does start to address some of these same scale problems, and you can actually hire Go developers. On the other hand, maybe the best of both worlds is to kick the concurrency can into the back yard of the cloud OS and let them deal with it for you. Then when your expensive Go or Erlang developer quits, it doesn’t destroy your entire company.</p>
<p class="noindent">Fortunately, though, with serverless technology, the weakness of Python on the Linux operating system suddenly becomes accidently irrelevant. A good example of this comes from my days running engineering at the Big Data company Loggly. We attempted to write a highly performant async-based Python log ingestion system in Python, and yes, running on one core it was pretty impressive, getting 6,000 to 8,000 requests per second. The problem, though, was that other cores sat idle, and the solution to start scaling that “async” Python collector to multiple cores ultimately wasn’t worth the engineering ROI. With the creation of serverless components from AWS, however, writing the entire system in Python is an excellent idea because the scalability is inherent to their platform.</p>
<p class="noindent">There is more to consider than just this with these new cloud operating systems. Many technologies like web frameworks are abstractions built on abstractions from decades previous. The relational database was invented in the 1970s, and yes, it is a solid technology—but in the early 2000s, web framework developers took this technology, which evolved in a PC and data center era, and slapped a bunch of web frameworks on top of it using object-relational mappers and code-generation tools. Almost by design, building web applications is an investment into legacy thought processes. They can absolutely solve problems, and they are powerful, but are they the future, especially within the context of large-scale AI projects? I would say no.</p>
<p class="noindent">Serverless technology is a completely different way of thinking. Databases can be self-scalable, and the schemas can be both flexible and efficient to manage. Instead of running web front ends like Apache or Nginx, which proxy down to code, there are stateless application servers that only run in response to events.</p>
<p class="noindent">Complexity isn’t free. With the complexity increasing with ML and AI applications, something has to give, and one way of decreasing complexity in an application is to no longer have servers that need to be maintained. This may also mean doing an <code class="calibre11">rm -rf</code> to traditional web frameworks. This won’t happen overnight though, so this chapter covers both a traditional web application, Flask, but with a cloud operating system twist to it. Other chapters have many examples of pure serverless architectures, specifically with AWS Chalice.</p>
<h3 id="ch05lev1" class="calibre12"><span epub:type="pagebreak" id="page_77" class="calibre2"></span>Building Augmented Reality and Virtual Reality Solutions on AWS</h3>
<p class="noindent">Working in the film industry and at Caltech gave me an appreciation for high-performance Linux file servers that were mounted across all workstations in the organization. It is very powerful to have thousands of machines and thousands of users all use a central mount point to configure the operating system, distribute data, and share disk I/O.</p>
<p class="noindent">A little known fact is that many film companies are ranked in the Top 500 list of supercomputers, and have been for years (<a href="https://www.top500.org/news/new-zealand-to-join-petaflop-club/" class="calibre7">https://www.top500.org/news/new-zealand-to-join-petaflop-club/</a>). This is because render farms, which point to high-performance centralized file servers, use tremendous computing and disk I/O resources. When I was at Weta Digital in New Zealand working on Avatar back in 2009 (<a href="https://www.geek.com/chips/the-computing-power-that-created-avatar-1031232/" class="calibre7">https://www.geek.com/chips/the-computing-power-that-created-avatar-1031232/</a>), they had 40,000 processors with 104TB of memory. They were processing up to 1.4 million tasks per day, so many film veterans laugh a little at current Spark and Hadoop-based workloads.</p>
<p class="noindent">The point in bringing up this story—outside of saying, “Get off my lawn!” to newcomers to Big Data—is that a centralized file server is a thing of beauty in large-scale computing. Historically, though, these “Ferrari” file servers took a whole crew of specialized “mechanics” to keep them running. With the cloud era, all of a sudden, you just point and click to get a Ferrari-based  file server.</p>
<h4 id="ch05lev1sub1" class="calibre16">Computer Vision: AR/VR Pipelines with EFS and Flask</h4>
<p class="noindent">AWS has a service Elastic File System (EFS), which is exactly that—a point-and-click “Ferrari” file server. One of the ways I have used it in the past was to create centralized file server for a virtual reality(VR)–based computer vision pipeline on AWS. The assets, code, and produced artifacts were all stored on EFS. In <a href="part0015.html#ch5fig1" class="calibre7">Figure 5.1</a>, this is what a VR pipeline on AWS could look like using EFS. The camera stations could involve 48 or 72 or more cameras all generating large frames that will later be ingested into a VR scene-stitching algorithm.</p>
<div class="figure">
<div class="image1"><span epub:type="pagebreak" id="page_78"></span><a id="ch5fig1" class="calibre7"></a><img src="../images/00022.jpeg" aria-describedby="alt_05fig01" alt="A figure of an A W S Virtual Reality Pipeline with E F S is shown." class="calibre8"/>
<aside class="hidden" id="alt_05fig01" data-AmznRemoved-M8="true" data-AmznRemoved="mobi7">
<p class="calibre21">The figure titled “High-Level Overview Virtual Reality Pipeline” shows two sections: Shoot Location and A W S Cloud on either side. The Shoot Location shows a Camera Stations (at the top) and a Recording Software (at the bottom) connected to an Ingest Frames (in the center). The A W S Cloud section shows E F S connected to a Render Software and from which it is connected to a Job Orchestration. The Job Orchestration is connected to a Node(s) Computer Vision Software which is classified into Node(s) Computer Vision Software and Node(s) Computer Vision Software. The Recording Software and the E F S are connected represents WAN to A W S Elastic File System (Large Scale Data Transfer).</p>
</aside>
</div>
<p class="fig_caption"><span class="calibre6">Figure 5.1</span> AWS VR Pipeline with EFS</p>
</div>
<p class="noindent">One subtle but powerful point is that EFS also makes deployment of Python application code a breeze because an EFS mount point can be created for each environment, say DEV, STAGE, and PRODUCTION. Then a “deployment” is an rsync of code, which can be done subsecond from the build server to the EFS mount point depending on the branch; say the DEV EFS mount point is the master branch, and the STAGE EFS mount point is the staging branch, etc. Then Flask will always have the latest version of code on disk, making deployment a trivial issue. An example of what that could look like in is <a href="part0015.html#ch5fig2" class="calibre7">Figure 5.2</a>.</p>
<div class="figure">
<div class="image1"><span epub:type="pagebreak" id="page_79"></span><a id="ch5fig2" class="calibre7"></a><img src="../images/00023.jpeg" aria-describedby="alt_05fig02" alt="A figure of detailed serverless Jobs Architecture with E F S is shown." class="calibre8"/>
<aside class="hidden" id="alt_05fig02" data-AmznRemoved-M8="true" data-AmznRemoved="mobi7">
<p class="calibre21">A rectangular box, AWS Lambda (Serverless) Microservices consists of Queue or Job Cleanup Service, Arbitration Services Dynamic Resizing of Tasks/Instances Ratio, Instance Shutdown Service, and Instance Tagging Service. An arrow from the left of the AWS Lambda (Serverless) Microservices box points out to the S N S Event Notification and an arrow from it points back to the AWS Lambda (Serverless) Microservices. On the right, a connection to the Slack Bot represents Key Events Propagated to Slack Bot is shown. On the left, SQS work Delegation, Chalice Job Control Web Application, and Job Submission A P I are shown connected via a bidirectional arrow respectively. Another bidirectional arrow is shown connecting the SQS work Delegation and the A W S Lambda (Serverless) Microservices. Three rectangular boxes: DynamoDB Job State Management Historical Job Performance Instance, Spot Instance Cluster Management, and Command-line Management Tools All Service Additionally Manageable via Command-line Tools are shown below. A bidirectional arrow is shown connecting the A W S Lambda (Serverless) Microservices and Spot Instance Cluster Management. Two bidirectional arrows from Job Submission A P I connected to DynamoDB Job State Management Historical Job Performance Instance and Command-line Management Tools All Service Additionally Manageable via Command-line Tools, respectively. A connection from the Spot Instance Cluster Management is shown to Elastic FileSystem and from which it is connected to Flask Internal Admin API (Controls EFS File System Operations) and Continuous Deployment. The connection between Elastic FileSystem and the Continuous Deployment represents Code Deployed to EFS in Seconds and Sourced via Environment.</p>
</aside>
</div>
<p class="fig_caption"><span class="calibre6">Figure 5.2</span> Detailed Serverless and Flask Architecture</p>
</div>
<p class="noindent">The ability to use serverless technology when it helps make things easier, plus EFS and Flask together is a powerful tool for building AI products of all kinds. In the examples shown, this was for a computer vision/VR/AR pipeline, but another place EFS comes in handy is for data engineering for traditional ML as well.</p>
<p class="noindent">Finally, one reason I know this architecture worked is that I wrote it from scratch for a VR/AR company, and it was so popular we quickly burned through 100,000 in credits from AWS in just a few months, spinning up giant multi-hundred-node jobs. Sometimes success does burn a hole in your pocket.</p>
<h4 id="ch05lev1sub2" class="calibre16">Data Engineering Pipeline with EFS, Flask, and Pandas</h4>
<p class="noindent">In building a production AI pipeline, data engineering can often be the biggest challenge. The following section will describe in detail how the start of a production API could be created in a company, like say, Netflix, AWS, or a unicorn startup. Data teams often need to build libraries and services to make it easier to work with data on the platform at their organization.</p>
<p class="noindent"><span epub:type="pagebreak" id="page_80"></span>In this example, there is a need to create a proof of concept aggregation of CSV data. A REST API that will accept a CSV file, a column to group on, and a column to aggregate returns the result. A further note on this example is that it is very real world in the sense that little details like API documentation, testing, continuous integration, plugins, and benchmarking are all included.</p>
<p class="noindent">The input to the problem should look like this.</p>
<p class="pre">first_name,last_name,count<br class="calibre9"/>
chuck,norris,10<br class="calibre9"/>
kristen,norris,17<br class="calibre9"/>
john,lee,3<br class="calibre9"/>
sam,mcgregor,15<br class="calibre9"/>
john,mcgregor,19</p>
<p class="noindent">When run against the API, it would then be</p>
<p class="pre">norris,27<br class="calibre9"/>
lee,3<br class="calibre9"/>
mcgregor,34</p>
<p class="noindent">The code for this entire project can be found here: <a href="https://github.com/noahgift/pai-aws" class="calibre7">https://github.com/noahgift/pai-aws</a>. Because using Makefiles and virtualenv are covered extensively in other chapters, this chapter will get right into the code. This project will contain five major pieces: the Flask app, a library “nlib,” notebooks, tests, and a command-line tool.</p>
<h5 class="calibre17">Flask App</h5>
<p class="noindent">The Flask app is composed of three components: static directory, which has a favicon.ico in it; a templates directory, which contains an index.html; and the core web application, which is about 150 lines of code. Here is a walkthrough of the core Flask app.</p>
<p class="noindent">This initial section imports Flask and flasgger (a swagger API doc generator; <a href="https://github.com/rochacbruno/flasgger" class="calibre7">https://github.com/rochacbruno/flasgger</a>), and defines logging and the Flask app.</p>
<p class="codelink"><a id="p080pro01" href="part0034_split_000.html#p080pro01a" class="calibre7">Click here to view code image</a></p>
<p class="pre">import os<br class="calibre9"/>
import base64<br class="calibre9"/>
import sys<br class="calibre9"/>
from io import BytesIO<br class="calibre9"/>
<br class="calibre9"/>
from flask import Flask<br class="calibre9"/>
from flask import send_from_directory<br class="calibre9"/>
from flask import request<br class="calibre9"/>
from flask_api import status<br class="calibre9"/>
from flasgger import Swagger<br class="calibre9"/>
from flask import redirect<br class="calibre9"/>
from flask import jsonify<br class="calibre9"/>
<br class="calibre9"/>
from sensible.loginit import logger<br class="calibre9"/>
from nlib import csvops<br class="calibre9"/>
from nlib import utils<br class="calibre9"/>
<br class="calibre9"/>
log = logger(__name__)<br class="calibre9"/>
<br class="calibre9"/>
app = Flask(__name__)<br class="calibre9"/>
Swagger(app)</p>
<p class="noindent"><span epub:type="pagebreak" id="page_81"></span>A helper function is created to do Base64 decoding of payloads.</p>
<p class="codelink"><a id="p081pro01" href="part0034_split_001.html#p081pro01a" class="calibre7">Click here to view code image</a></p>
<p class="pre">def _b64decode_helper(request_object):<br class="calibre9"/>
    """Returns base64 decoded data and size of encoded data"""<br class="calibre9"/>
<br class="calibre9"/>
    size=sys.getsizeof(request_object.data)<br class="calibre9"/>
    decode_msg = "Decoding data of size: {size}".format(size=size)<br class="calibre9"/>
    log.info(decode_msg)<br class="calibre9"/>
    decoded_data = BytesIO(base64.b64decode(request.data))<br class="calibre9"/>
    return decoded_data, size</p>
<p class="noindent">Next, a couple of routes are created that are really boilerplate routes that serve out the favicon and redirect to the main docs.</p>
<p class="codelink"><a id="p081pro02" href="part0034_split_002.html#p081pro02a" class="calibre7">Click here to view code image</a></p>
<p class="pre">@app.route("/")<br class="calibre9"/>
def home():<br class="calibre9"/>
    """/ Route will redirect to API Docs: /apidocs"""<br class="calibre9"/>
<br class="calibre9"/>
    return redirect("/apidocs")<br class="calibre9"/>
<br class="calibre9"/>
<br class="calibre9"/>
@app.route("/favicon.ico")<br class="calibre9"/>
def favicon():<br class="calibre9"/>
    """The Favicon"""<br class="calibre9"/>
<br class="calibre9"/>
    return send_from_directory(os.path.join(app.root_path, 'static'),<br class="calibre9"/>
                    'favicon.ico',<br class="calibre9"/>
                    mimetype='image/vnd.microsoft.icon')</p>
<p class="noindent">Things start to get more interesting with the /api/funcs route. This lists dynamically installable plugins. These could be custom algorithms. This will be described in more detail in the library section.</p>
<p class="codelink"><a id="p081pro03" href="part0034_split_003.html#p081pro03a" class="calibre7">Click here to view code image</a></p>
<p class="pre">@app.route('/api/funcs', methods = ['GET'])<br class="calibre9"/>
def list_apply_funcs():<br class="calibre9"/>
    """Return a list of appliable functions<br class="calibre9"/>
<br class="calibre9"/>
<br class="calibre9"/>
        GET /api/funcs<br class="calibre9"/>
        ---<br class="calibre9"/>
        responses:<br class="calibre9"/>
            200:<br class="calibre9"/>
                description: Returns list of appliable functions.<br class="calibre9"/>
<br class="calibre9"/>
<br class="calibre9"/>
    """<br class="calibre9"/>
<br class="calibre9"/>
    appliable_list = utils.appliable_functions()<br class="calibre9"/>
    return jsonify({"funcs":appliable_list})</p>
<p class="noindent"><span epub:type="pagebreak" id="page_82"></span>This section creates a groupby route and it contains detailed docstring documentation, so that swagger API docs can be dynamically created.</p>
<p class="codelink"><a id="p082pro01" href="part0034_split_004.html#p082pro01a" class="calibre7">Click here to view code image</a></p>
<p class="pre">@app.route('/api/&lt;groupbyop&gt;', methods = ['PUT'])<br class="calibre9"/>
def csv_aggregate_columns(groupbyop):<br class="calibre9"/>
    """Aggregate column in an uploaded csv<br class="calibre9"/>
    <br class="calibre9"/>
    ---<br class="calibre9"/>
        consumes:  application/json<br class="calibre9"/>
        parameters:<br class="calibre9"/>
            -   in: path<br class="calibre9"/>
                name:  Appliable Function (i.e.  npsum, npmedian)<br class="calibre9"/>
                type:  string<br class="calibre9"/>
                required: true<br class="calibre9"/>
                description:  appliable function,<br class="calibre9"/>
                 which must be registered (check /api/funcs)<br class="calibre9"/>
            -   in: query<br class="calibre9"/>
                name: column<br class="calibre9"/>
                type: string<br class="calibre9"/>
                description:  The column to process in an aggregation<br class="calibre9"/>
                required:  True<br class="calibre9"/>
            -   in: query    <br class="calibre9"/>
                name: group_by<br class="calibre9"/>
                type: string<br class="calibre9"/>
                description:\<br class="calibre9"/>
                   The column to group_by in an aggregation<br class="calibre9"/>
                required:  True<br class="calibre9"/>
            -   in: header<br class="calibre9"/>
                name:  Content-Type<br class="calibre9"/>
                type:  string<br class="calibre9"/>
                description: \<br class="calibre9"/>
                   Requires "Content-Type:application/json" to be set<br class="calibre9"/>
                required:  True<br class="calibre9"/>
            -   in: body<br class="calibre9"/>
                name: payload<br class="calibre9"/>
                type:  string<br class="calibre9"/>
                description:  base64 encoded csv file<br class="calibre9"/>
                required: True<br class="calibre9"/>
<br class="calibre9"/>
        responses:<br class="calibre9"/>
            200:<br class="calibre9"/>
                description: Returns an aggregated CSV.<br class="calibre9"/>
<br class="calibre9"/>
    """</p>
<p class="noindent">Finally, the meat of the API call is created below. Note that many “messy” real-world problems are addressed here, like ensuring the correct content type, looking for a specific HTTP method, logging dynamically loading plugins, and returning a correct JSON response with a 200 Status Code if things are correct, but returning other HTTP status codes if they are incorrect.</p>
<p class="codelink"><a id="p082pro02" href="part0034_split_005.html#p082pro02a" class="calibre7">Click here to view code image</a></p>
<p class="pre">    content_type = request.headers.get('Content-Type')<br class="calibre9"/>
    content_type_log_msg =\<br class="calibre9"/>
         "Content-Type is set to:  {content_type}".\<br class="calibre9"/>
        format(content_type=content_type)<br class="calibre9"/>
    log.info(content_type_log_msg)<br class="calibre9"/>
    if not content_type == "application/json":<br class="calibre9"/>
        wrong_method_log_msg =\<br class="calibre9"/>
             "Wrong Content-Type in request:\<br class="calibre9"/>
          {content_type} sent, but requires application/json".\<br class="calibre9"/>
            format(content_type=content_type)<br class="calibre9"/>
        log.info(wrong_method_log_msg)<br class="calibre9"/>
        return jsonify({"content_type": content_type,<br class="calibre9"/>
                "error_msg": wrong_method_log_msg}),<br class="calibre9"/>  status.HTTP_415_UNSUPPORTED_MEDIA_TYPE<br class="calibre9"/>
<br class="calibre9"/>
    #Parse Query Parameters and Retrieve Values<br class="calibre9"/>
    query_string = request.query_string<br class="calibre9"/>
    query_string_msg = "Request Query String:<br class="calibre9"/>  {query_string}".format(query_string=query_string)<br class="calibre9"/>
    log.info(query_string_msg)<br class="calibre9"/>
    column = request.args.get("column")<br class="calibre9"/>
    group_by = request.args.get("group_by")<br class="calibre9"/>
    <br class="calibre9"/>
    #Query Parameter logging and handling<br class="calibre9"/>
    query_parameters_log_msg =\<br class="calibre9"/>
        "column: [{column}] and group_by:\<br class="calibre9"/>
        [{group_by}] Query Parameter values".\<br class="calibre9"/>
        format(column=column, group_by=group_by)<br class="calibre9"/>
    log.info(query_parameters_log_msg)<br class="calibre9"/>
    if not column or not group_by:<br class="calibre9"/>
        error_msg = "Query Parameter column or group_by not set"<br class="calibre9"/>
        log.info(error_msg)<br class="calibre9"/>
        return jsonify({"column": column, "group_by": group_by,<br class="calibre9"/>
                "error_msg": error_msg}), status.HTTP_400_BAD_REQUEST<br class="calibre9"/>
<br class="calibre9"/>
    #Load Plugins and grab correct one<br class="calibre9"/>
    plugins = utils.plugins_map()<br class="calibre9"/>
    appliable_func = plugins[groupbyop]<br class="calibre9"/>
<br class="calibre9"/>
    #Unpack data and operate on it<br class="calibre9"/>
    data,_ = _b64decode_helper(request)<br class="calibre9"/>
    #Returns Pandas Series<br class="calibre9"/>
    res = csvops.group_by_operations(data,<br class="calibre9"/>
        groupby_column_name=group_by, \<br class="calibre9"/>
        apply_column_name=column, func=appliable_func)<br class="calibre9"/>
    log.info(res)<br class="calibre9"/>
    return res.to_json(), status.HTTP_200_OK</p>
<p class="noindent"><span epub:type="pagebreak" id="page_83"></span>This code block sets flags like debug and includes boilerplate code to run a Flask app as a script.</p>
<p class="codelink"><a id="p083pro01" href="part0034_split_007.html#p083pro01a" class="calibre7">Click here to view code image</a></p>
<p class="pre">if __name__ == "__main__": # pragma: no cover<br class="calibre9"/>
    log.info("START Flask")<br class="calibre9"/>
    app.debug = True<br class="calibre9"/>
    app.run(host='0.0.0.0', port=5001)<br class="calibre9"/>
    log.info("SHUTDOWN Flask")</p>
<p class="noindent"><span epub:type="pagebreak" id="page_84"></span>Next to run the app, I created a Makefile command as shown.</p>
<p class="codelink"><a id="p084pro01" href="part0034_split_008.html#p084pro01a" class="calibre7">Click here to view code image</a></p>
<p class="pre">(.pia-aws) →  pai-aws git:(master) make start-api<br class="calibre9"/>
#sets PYTHONPATH to directory above,<br class="calibre9"/>
#would do differently in production<br class="calibre9"/>
cd flask_app &amp;&amp; PYTHONPATH=".." python web.py<br class="calibre9"/>
2018-03-17 19:14:59,807 - __main__ - INFO - START Flask<br class="calibre9"/>
 * Running on http://0.0.0.0:5001/ (Press CTRL+C to quit)<br class="calibre9"/>
 * Restarting with stat<br class="calibre9"/>
2018-03-17 19:15:00,475 - __main__ - INFO - START Flask<br class="calibre9"/>
 * Debugger is active!<br class="calibre9"/>
 * Debugger PIN: 171-594-84</p>
<p class="noindent">In <a href="part0015.html#ch5fig3" class="calibre7">Figure 5.3</a>, the swagger documentation conveniently lets a user list the available funcs, that is, plugins from nlib. The output shows that there is a npmedian, npsum, numpy, and tanimoto function loaded. In <a href="part0015.html#ch5fig4" class="calibre7">Figure 5.4</a>, there is a useful web form that allows a developer to fully complete the API call without using curl or a programming language. The really powerful thing about this system is that the core web app was only 150 lines of code, yet is real world and about ready for production!</p>
<div class="figure">
<div class="image1"><img src="../images/00024.jpeg" aria-describedby="alt_05fig03" alt="A screenshot displays the Listing of Plugins Available." class="calibre8"/>
<aside class="hidden" id="alt_05fig03" data-AmznRemoved-M8="true" data-AmznRemoved="mobi7">
<p class="calibre21">The screen, default shows GET tab selected at the top. The GET displays, Response Messages section that reads the following column header: HTTP Status Code, Reason, Response Model, and Headers. The following data under the column headers read 200, Returns list of applicable functions, and the other two are left blank. The Curl section below reads, curl –X GET –header ‘Accept: application/json’ ‘http://0.0.0.0:5001/api/funcs.’ The Request URL section reads, http://0.0.0.0:5001/api/funcs. The Response Body section consists of 8 lines of coding, The Response Code reads, 200. The Response Headers consists of 6 lines of coding.</p>
</aside>
</div>
<p class="fig_caption"><a id="ch5fig3" class="calibre7"></a><span class="calibre6">Figure 5.3</span> Listing of Plugins Available</p>
</div>
<div class="figure">
<div class="image1"><img src="../images/00025.jpeg" aria-describedby="alt_05fig04" alt="A screenshot of a webpage form to fully complete the API call without using curl or a programming language." class="calibre8"/>
<aside class="hidden" id="alt_05fig04" data-AmznRemoved-M8="true" data-AmznRemoved="mobi7">
<p class="calibre21">The screen displays, the web address: 0.0.0.0:5001/apidocs/#/. The name of the page, Flasgger is shown at the top of the screen and a text box reads the following address: http://0.0.0.0:5001/apispec_1.json with an Explore command bottom. The page displays, A swagger A P I, powered by Flasgger. The default section shows the PUT tab selected and the following are displayed. The parameter table is shown below that reads the following column header: Parameter, Value, Description, Parameter Type, and Data Type. The value column is requested to be filled by the developer (textboxes) are shown. The Parameter content type spin box shown below the table is set to “application/json. The Response Messages section that reads the following column header: HTTP Status Code, Reason, Response Model, and Headers are shown.</p>
</aside>
</div>
<p class="fig_caption"><a id="ch5fig4" class="calibre7"></a><span class="calibre6">Figure 5.4</span> Using the API</p>
</div>
<h5 class="calibre17"><span epub:type="pagebreak" id="page_85"></span>Library and Plugins</h5>
<p class="noindent">Inside of the nlib directory there are four files: __init__.py, applicable.py, csvops.py, and utils.py. Here is a breakdown of each file.</p>
<p class="noindent">The __init__.py is very simple, it contains a version variable.</p>
<p class="pre">__version__ = 0.1</p>
<p class="noindent">Next is the utils.py file, which is a plugin loader and it finds “appliable” functions from  appliable.py file.</p>
<p class="codelink"><a id="p085pro01" href="part0034_split_009.html#p085pro01a" class="calibre7">Click here to view code image</a></p>
<p class="pre">"""Utilities<br class="calibre9"/>
<br class="calibre9"/>
<br class="calibre9"/>
<span epub:type="pagebreak" id="page_86"></span>Main use it to serve as a 'plugins' utility so that functions can be:<br class="calibre9"/>
    * registered<br class="calibre9"/>
    * discovered<br class="calibre9"/>
    * documented<br class="calibre9"/>
<br class="calibre9"/>
"""<br class="calibre9"/>
<br class="calibre9"/>
import importlib<br class="calibre9"/>
<br class="calibre9"/>
from sensible.loginit import logger<br class="calibre9"/>
<br class="calibre9"/>
log = logger(__name__)<br class="calibre9"/>
<br class="calibre9"/>
def appliable_functions():<br class="calibre9"/>
    """Returns a list of appliable functions<br class="calibre9"/>
        to be used in GroupBy Operations"""<br class="calibre9"/>
<br class="calibre9"/>
    from . import appliable<br class="calibre9"/>
    module_items = dir(appliable)<br class="calibre9"/>
    #Filter out special items __<br class="calibre9"/>
    func_list = list(<br class="calibre9"/>
         filter(lambda x: not x.startswith("__"),<br class="calibre9"/>
         module_items))<br class="calibre9"/>
    return func_list<br class="calibre9"/>
<br class="calibre9"/>
<br class="calibre9"/>
def plugins_map():<br class="calibre9"/>
    """Create a dictionary of callable functions<br class="calibre9"/>
<br class="calibre9"/>
    In [2]: plugins = utils.plugins_map()<br class="calibre9"/>
Loading appliable functions/plugins: npmedian<br class="calibre9"/>
Loading appliable functions/plugins: npsum<br class="calibre9"/>
Loading appliable functions/plugins: numpy<br class="calibre9"/>
Loading appliable functions/plugins: tanimoto<br class="calibre9"/>
<br class="calibre9"/>
    In [3]: plugins<br class="calibre9"/>
    Out[3]:<br class="calibre9"/>
    {'npmedian': &lt;function nlib.appliable.npmedian&gt;,<br class="calibre9"/>
    'npsum': &lt;function nlib.appliable.npsum&gt;,<br class="calibre9"/>
    'numpy': &lt;module 'numpy' from site-packages...&gt;,<br class="calibre9"/>
    'tanimoto': &lt;function nlib.appliable.tanimoto&gt;}<br class="calibre9"/>
<br class="calibre9"/>
    In [4]: plugins['npmedian']([1,3])<br class="calibre9"/>
    Out[4]: 2.0<br class="calibre9"/>
    """<br class="calibre9"/>
<br class="calibre9"/>
    plugins = {}<br class="calibre9"/>
    funcs = appliable_functions()<br class="calibre9"/>
    for func in funcs:<br class="calibre9"/>
        plugin_load_msg =\<br class="calibre9"/>
          "Loading appliable functions/plugins:\<br class="calibre9"/>
          {func}".format(func=func)<br class="calibre9"/>
        log.info(plugin_load_msg)<br class="calibre9"/>
        plugins[func] = getattr(<br class="calibre9"/>
        importlib.import_module("nlib.appliable"), func<br class="calibre9"/>
        )<br class="calibre9"/>
    return plugins</p>
<p class="noindent"><span epub:type="pagebreak" id="page_87"></span>The appliable.py file is where custom functions can be created. These functions are “applied” to column in a Pandas DataFrame and could be completely customized to do anything that  can be done to a column.</p>
<p class="codelink"><a id="p087pro01" href="part0034_split_011.html#p087pro01a" class="calibre7">Click here to view code image</a></p>
<p class="pre">"""Appliable Functions to a Pandas GroupBy Operation (I.E Plugins)"""<br class="calibre9"/>
<br class="calibre9"/>
import numpy<br class="calibre9"/>
<br class="calibre9"/>
<br class="calibre9"/>
def tanimoto(list1, list2):<br class="calibre9"/>
    """tanimoto coefficient<br class="calibre9"/>
<br class="calibre9"/>
    In [2]: list2=['39229', '31995', '32015']<br class="calibre9"/>
    In [3]: list1=['31936', '35989', '27489',<br class="calibre9"/>
        '39229', '15468', '31993', '26478']<br class="calibre9"/>
    In [4]: tanimoto(list1,list2)<br class="calibre9"/>
    Out[4]: 0.1111111111111111<br class="calibre9"/>
<br class="calibre9"/>
    Uses intersection of two sets to determine numerical score<br class="calibre9"/>
<br class="calibre9"/>
    """<br class="calibre9"/>
<br class="calibre9"/>
    intersection = set(list1).intersection(set(list2))<br class="calibre9"/>
    return float(len(intersection))\<br class="calibre9"/>
         /(len(list1) + len(list2) - len(intersection))<br class="calibre9"/>
<br class="calibre9"/>
def npsum(x):<br class="calibre9"/>
    """Numpy Library Sum"""<br class="calibre9"/>
<br class="calibre9"/>
    return numpy.sum(x)<br class="calibre9"/>
<br class="calibre9"/>
def npmedian(x):<br class="calibre9"/>
    """Numpy Library Median"""<br class="calibre9"/>
<br class="calibre9"/>
    return numpy.median(x)</p>
<p class="noindent">Finally, the cvops module deals with csv ingest and operations as shown.</p>
<p class="codelink"><a id="p087pro01z" href="part0034_split_012.html#p087pro01zz" class="calibre7">Click here to view code image</a></p>
<p class="pre">"""<br class="calibre9"/>
<span epub:type="pagebreak" id="page_88"></span>CSV Operations Module:<br class="calibre9"/>
See this for notes on I/O Performance in Pandas:<br class="calibre9"/>
    http://pandas.pydata.org/pandas-docs/stable/io.html#io-perf<br class="calibre9"/>
"""<br class="calibre9"/>
<br class="calibre9"/>
from sensible.loginit import logger<br class="calibre9"/>
import pandas as pd<br class="calibre9"/>
<br class="calibre9"/>
log = logger(__name__)<br class="calibre9"/>
log.debug("imported csvops module")<br class="calibre9"/>
<br class="calibre9"/>
<br class="calibre9"/>
def ingest_csv(data):<br class="calibre9"/>
    """Ingests a CSV using Pandas CSV I/O"""<br class="calibre9"/>
<br class="calibre9"/>
    df = pd.read_csv(data)<br class="calibre9"/>
    return df<br class="calibre9"/>
<br class="calibre9"/>
def list_csv_column_names(data):<br class="calibre9"/>
    """Returns a list of column names from csv"""<br class="calibre9"/>
<br class="calibre9"/>
    df = ingest_csv(data)<br class="calibre9"/>
    colnames = list(df.columns.values)<br class="calibre9"/>
    colnames_msg = "Column Names: {colnames}".\<br class="calibre9"/>
        format(colnames=colnames)<br class="calibre9"/>
    log.info(colnames_msg)<br class="calibre9"/>
    return colnames<br class="calibre9"/>
<br class="calibre9"/>
def aggregate_column_name(data,<br class="calibre9"/>
         groupby_column_name, apply_column_name):<br class="calibre9"/>
    """Returns aggregated results of csv by column name as json"""<br class="calibre9"/>
<br class="calibre9"/>
    <br class="calibre9"/>
    df = ingest_csv(data)<br class="calibre9"/>
    res = df.groupby(groupby_column_name)[apply_column_name].sum()<br class="calibre9"/>
    return res<br class="calibre9"/>
<br class="calibre9"/>
def group_by_operations(data,<br class="calibre9"/>
        groupby_column_name, apply_column_name, func):<br class="calibre9"/>
    """<br class="calibre9"/>
    <br class="calibre9"/>
    Allows a groupby operation to take arbitrary functions<br class="calibre9"/>
    <br class="calibre9"/>
    In [14]: res_sum = group_by_operations(data=data,<br class="calibre9"/>
        groupby_column_name="last_name", columns="count",<br class="calibre9"/>
        func=npsum)<br class="calibre9"/>
    In [15]: res_sum<br class="calibre9"/>
    Out[15]:<br class="calibre9"/>
    last_name<br class="calibre9"/>
    eagle    34<br class="calibre9"/>
    lee       3<br class="calibre9"/>
    smith    27<br class="calibre9"/>
    Name: count, dtype: int64<br class="calibre9"/>
    """<br class="calibre9"/>
<br class="calibre9"/>
    df = ingest_csv(data)<br class="calibre9"/>
    grouped = df.groupby(groupby_column_name)[apply_column_name]<br class="calibre9"/>
    #GroupBy with filter to specific column(s)<br class="calibre9"/>
    applied_data = grouped.apply(func)<br class="calibre9"/>
    return applied_data</p>
<h5 class="calibre17"><span epub:type="pagebreak" id="page_89"></span>Command-line Tool</h5>
<p class="noindent">In yet another “get off my lawn” moment, I am going to create a command-line tool because I believe they are useful in just about any project. Despite how powerful Jupyter Notebooks are, there things that command-line tools just do better.</p>
<p class="noindent">Here is what cvscli.py looks like. At the start, boilerplate documentation and imports are created.</p>
<p class="codelink"><a id="p089pro01" href="part0034_split_015.html#p089pro01a" class="calibre7">Click here to view code image</a></p>
<p class="pre">#!/usr/bin/env python<br class="calibre9"/>
"""<br class="calibre9"/>
Commandline Tool For Doing CSV operations:<br class="calibre9"/>
<br class="calibre9"/>
    * Aggregation<br class="calibre9"/>
    * TBD<br class="calibre9"/>
<br class="calibre9"/>
"""<br class="calibre9"/>
<br class="calibre9"/>
import sys<br class="calibre9"/>
<br class="calibre9"/>
import click<br class="calibre9"/>
from sensible.loginit import logger<br class="calibre9"/>
<br class="calibre9"/>
import nlib<br class="calibre9"/>
from nlib import csvops<br class="calibre9"/>
from nlib import utils<br class="calibre9"/>
<br class="calibre9"/>
log = logger(__name__)</p>
<p class="noindent">Next, the meat of the command-line tool does the same thing as the HTTP API. There is documentation enclosed and a sample file in ext/input.csv that allows the tool be tested. The output is included in the docstring to be helpful to a user of the tool.</p>
<p class="codelink"><a id="p089pro02" href="part0034_split_016.html#p089pro02a" class="calibre7">Click here to view code image</a></p>
<p class="pre">@click.version_option(nlib.__version__)<br class="calibre9"/>
@click.group()<br class="calibre9"/>
def cli():<br class="calibre9"/>
    """CSV Operations Tool<br class="calibre9"/>
<br class="calibre9"/>
    """<br class="calibre9"/>
<span epub:type="pagebreak" id="page_90"></span>
@cli.command("cvsops")<br class="calibre9"/>
@click.option('--file', help='Name of csv file')<br class="calibre9"/>
@click.option('--groupby', help='GroupBy Column Name')<br class="calibre9"/>
@click.option('--applyname', help='Apply Column Name')<br class="calibre9"/>
@click.option('--func', help='Appliable Function')<br class="calibre9"/>
def agg(file,groupby, applyname, func):<br class="calibre9"/>
    """Operates on a groupby column in a csv file<br class="calibre9"/>
 and applies a function<br class="calibre9"/>
<br class="calibre9"/>
    Example Usage:<br class="calibre9"/>
   ./csvcli.py cvsops --file ext/input.csv –groupby\<br class="calibre9"/>
 last_name --applyname count --func npmedian<br class="calibre9"/>
    Processing csvfile: ext/input.csv and groupby name:\<br class="calibre9"/>
 last_name and applyname: count<br class="calibre9"/>
    2017-06-22 14:07:52,532 - nlib.utils - INFO - \<br class="calibre9"/>
Loading appliable functions/plugins: npmedian<br class="calibre9"/>
    2017-06-22 14:07:52,533 - nlib.utils - INFO - \<br class="calibre9"/>
Loading appliable functions/plugins: npsum<br class="calibre9"/>
    2017-06-22 14:07:52,533 - nlib.utils - INFO - \<br class="calibre9"/>
Loading appliable functions/plugins: numpy<br class="calibre9"/>
    2017-06-22 14:07:52,533 - nlib.utils - INFO - \<br class="calibre9"/>
Loading appliable functions/plugins: tanimoto<br class="calibre9"/>
    last_name<br class="calibre9"/>
    eagle    17.0<br class="calibre9"/>
    lee       3.0<br class="calibre9"/>
    smith    13.5<br class="calibre9"/>
    Name: count, dtype: float64<br class="calibre9"/>
<br class="calibre9"/>
    """<br class="calibre9"/>
    if not file and not groupby and not applyname and not func:<br class="calibre9"/>
        click.echo("--file and --column and –applyname\<br class="calibre9"/>
 --func are required")<br class="calibre9"/>
        sys.exit(1)<br class="calibre9"/>
<br class="calibre9"/>
    click.echo("Processing csvfile: {file} and groupby name:\<br class="calibre9"/>
 {groupby} and applyname: {applyname}".\<br class="calibre9"/>
            format(file=file, groupby=groupby, applyname=applyname))<br class="calibre9"/>
    #Load Plugins and grab correct one<br class="calibre9"/>
    plugins = utils.plugins_map()<br class="calibre9"/>
    appliable_func = plugins[func]<br class="calibre9"/>
    res = csvops.group_by_operations(data=file,<br class="calibre9"/>
            groupby_column_name=groupby, apply_column_name=applyname,<br class="calibre9"/>
            func=appliable_func)<br class="calibre9"/>
    click.echo(res)</p>
<p class="noindent">Finally, just the web api, the command-line tool allows a user to list the plugins that are available.</p>
<p class="codelink"><a id="p090pro01" href="part0034_split_018.html#p090pro01a" class="calibre7">Click here to view code image</a></p>
<p class="pre">@cli.command("listfuncs")<br class="calibre9"/>
def listfuncs():<br class="calibre9"/>
    """Lists functions that can be applied to a GroupBy Operation<br class="calibre9"/>
<br class="calibre9"/>
    Example Usage:<br class="calibre9"/>
<br class="calibre9"/>
    ./csvcli.py listfuncs<br class="calibre9"/>
    Appliable Functions: ['npmedian', 'npsum', 'numpy', 'tanimoto']<br class="calibre9"/>
    """<br class="calibre9"/>
<br class="calibre9"/>
    funcs = utils.appliable_functions()<br class="calibre9"/>
    click.echo("Appliable Functions: {funcs}".format(funcs=funcs))<br class="calibre9"/>
<br class="calibre9"/>
<br class="calibre9"/>
if __name__ == "__main__":<br class="calibre9"/>
    cli()</p>
<h5 class="calibre17"><span epub:type="pagebreak" id="page_91"></span>Benchmarking and Testing the API</h5>
<p class="noindent">When creating production APIs in the real world, it would be shameful to not do some benchmarking before putting them into production. Here is what that looks like via a Makefile command.</p>
<p class="codelink"><a id="p090pro02" href="part0034_split_019.html#p090pro02a" class="calibre7">Click here to view code image</a></p>
<p class="pre">→  pai-aws git:(master) make benchmark-web-sum<br class="calibre9"/>
#very simple benchmark of api on sum operations<br class="calibre9"/>
ab -n 1000 -c 100 -T 'application/json' -u ext/input_base64.txt\<br class="calibre9"/>
http://0.0.0.0:5001/api/npsum\?column=count\&amp;group_by=last_name<br class="calibre9"/>
This is ApacheBench, Version 2.3 &lt;$Revision: 1757674 $&gt;<br class="calibre9"/>
……<br class="calibre9"/>
Benchmarking 0.0.0.0 (be patient)<br class="calibre9"/>
Completed 100 requests<br class="calibre9"/>
Finished 1000 requests<br class="calibre9"/>
<br class="calibre9"/>
<br class="calibre9"/>
Server Software:        Werkzeug/0.14.1<br class="calibre9"/>
Server Hostname:        0.0.0.0<br class="calibre9"/>
Server Port:            5001<br class="calibre9"/>
<br class="calibre9"/>
Document Path:          /api/npsum?column=count&amp;group_by=last_name<br class="calibre9"/>
Document Length:        31 bytes<br class="calibre9"/>
<br class="calibre9"/>
Concurrency Level:      100<br class="calibre9"/>
Time taken for tests:   4.105 seconds<br class="calibre9"/>
Complete requests:      1000<br class="calibre9"/>
Failed requests:        0<br class="calibre9"/>
Total transferred:      185000 bytes<br class="calibre9"/>
Total body sent:        304000<br class="calibre9"/>
HTML transferred:       31000 bytes<br class="calibre9"/>
Requests per second:    243.60 [#/sec] (mean)<br class="calibre9"/>
Time per request:       410.510 [ms] (mean)</p>
<p class="noindent">In this case, the application has a reasonable performance for what it does, and it will scale reasonably well behind an Elastic Load Balancer (ELB) with multiple Nginx nodes. However, it should be pointed out that yes, this is one of those examples of how powerful and fun Python is to code with, but also how languages like C++, Java, C#, and Go blow it away in terms of <span epub:type="pagebreak" id="page_92"></span>performance. It isn’t uncommon for an Erlang or Go application to be doing something similar and getting thousands of requests per second.</p>
<p class="noindent">In this case, though, the speed of what was developed and the specific data science use case make it a reasonable tradeoff, for now. Version two ideas include switching this over to AWS Chalice and using something like Spark and/or Redis to cache requests and store results in memory. Note that AWS Chalice also has the ability to do API request caching by default, so adding several layers of caching would be fairly trivial.</p>
<h5 class="calibre17">Ideas for Deploying into EFS</h5>
<p class="noindent">The final thing to do to deploy this into production is have a build server that mounts several EFS mount points: one for the development environment, one for production, etc. When code is pushed out to a branch, a build job rsyncs it to the correct mount point. One way to add some smarts into code to ensure it knows where the correct environments are is to use the EFS name as a way to route to an environment. Here is what that could look like with a file called env.py.</p>
<p class="noindent">By hacking the <code class="calibre11">df</code> command on Linux, the code can always be sure it is running in the right spot. A further improvement could be to store the ENV data in the AWS Systems Manager Parameter Store (<a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html" class="calibre7">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html</a>).</p>
<p class="codelink"><a id="p092pro01" href="part0034_split_020.html#p092pro01a" class="calibre7">Click here to view code image</a></p>
<p class="pre">"""<br class="calibre9"/>
Environmental Switching Code:<br class="calibre9"/>
<br class="calibre9"/>
    Assumptions here are that EFS is essentially a key to map off of<br class="calibre9"/>
"""<br class="calibre9"/>
<br class="calibre9"/>
from subprocess import Popen, PIPE<br class="calibre9"/>
<br class="calibre9"/>
ENV = {<br class="calibre9"/>
    "local": {"file_system_id": "fs-999BOGUS",\<br class="calibre9"/>
        "tools_path": ".."}, #used for testing<br class="calibre9"/>
    "dev": {"file_system_id": "fs-203cc189"},<br class="calibre9"/>
    "prod": {"file_system_id": "fs-75bc4edc"}<br class="calibre9"/>
}<br class="calibre9"/>
<br class="calibre9"/>
def df():<br class="calibre9"/>
    """Gets df output"""<br class="calibre9"/>
    <br class="calibre9"/>
    p = Popen('df', stdin=PIPE, stdout=PIPE, stderr=PIPE)<br class="calibre9"/>
    output, err = p.communicate()<br class="calibre9"/>
    rc = p.returncode<br class="calibre9"/>
    if rc == 0:<br class="calibre9"/>
        return output<br class="calibre9"/>
    return rc,err<br class="calibre9"/>
<br class="calibre9"/>
def get_amazon_path(dfout):<br class="calibre9"/>
    """Grab the amazon path out of a disk mount"""<br class="calibre9"/>
    <br class="calibre9"/>
<span epub:type="pagebreak" id="page_93"></span>
    for line in dfout.split():<br class="calibre9"/>
        if "amazonaws" in line:<br class="calibre9"/>
            return line<br class="calibre9"/>
    return False<br class="calibre9"/>
<br class="calibre9"/>
def get_env_efsid(local=False):<br class="calibre9"/>
    """Parses df to get env and efs id"""<br class="calibre9"/>
    <br class="calibre9"/>
    if local:<br class="calibre9"/>
        return ("local", ENV["local"]["file_system_id"])<br class="calibre9"/>
    dfout = df()<br class="calibre9"/>
    path = get_amazon_path(dfout)<br class="calibre9"/>
    for key, value in ENV.items():<br class="calibre9"/>
        env = key<br class="calibre9"/>
        efsid = value["file_system_id"]<br class="calibre9"/>
        if path:<br class="calibre9"/>
            if efsid in path:<br class="calibre9"/>
                return (env, efsid)<br class="calibre9"/>
    return False<br class="calibre9"/>
<br class="calibre9"/>
def main():<br class="calibre9"/>
    env, efsid = get_env_efsid()<br class="calibre9"/>
    print "ENVIRONMENT: %s | EFS_ID: %s" % (env,efsid)<br class="calibre9"/>
<br class="calibre9"/>
if __name__ == '__main__':<br class="calibre9"/>
    main()%</p>
<h3 id="ch05lev2" class="calibre12">Summary</h3>
<p class="noindent">AWS is a very reasonable choice around which to base the technology decisions for a company. Just from looking at the market cap alone of Amazon, it will be innovating and lowering costs for quite some time. If you look at what they have done with serverless technology, it becomes very exciting.</p>
<p class="noindent">It is easy to get caught up in being deeply worried about vendor lock-in, but using Erlang on DigitalOcean or your data center doesn’t mean you aren’t locked into a vendor. You are locked into your small, exotic development team, or your systems admins.</p>
<p class="noindent">This chapter showed real-world APIs and solutions based on problems I have solved in consulting on AWS. Many of the other ideas in chapters in this book could be hooked into ideas from this chapter, and the marriage could be a production solution.<span epub:type="pagebreak" id="page_94"></span></p>
</body></html>
