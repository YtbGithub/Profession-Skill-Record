<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:ns="http://www.w3.org/2001/10/synthesis" xml:lang="en-us" lang="en-us">
  <head>
    <title>2 AI and ML Toolchain</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="../stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<h2 class="h1" id="ch02"><span epub:type="pagebreak" id="page_27" class="calibre2"></span>2</h2>
<h2 class="h2a">AI and ML Toolchain</h2>
<p class="blockquote"><em class="calibre5">We’re talking about practice. We’re talking about practice. We ain’t talking about the game</em>.</p>
<p class="attribution">Allen Iverson</p>
<p class="noindent">There is a growing wave of articles, videos, courses, and degrees in AI and machine learning (ML). What isn’t covered as much is the toolchain. What are the fundamental skills needed to be a data scientist creating production machine learning? What are the fundamental processes a company must establish to develop reliable automated systems that can make predictions?</p>
<p class="noindent">The toolchain needed to be successful in data science has exploded in complexity in some areas and shrunk in others. Jupyter Notebook is one of the innovations that has shrunk the complexity of developing solutions, as have cloud computing and SaaS build systems. The philosophy of DevOps touches on many areas including data security and reliability. The 2017 Equifax hack and the 2018 Facebook/Cambridge Analytica scandal that released the information of a significant portion of Americans highlights that data security is one of most pressing issues of our time. This chapter covers these issues and presents recommendations on how to develop processes that increase the reliability and security of ML systems.</p>
<h3 id="ch02lev1" class="calibre12">Python Data Science Ecosystem: IPython, Pandas, NumPy, Jupyter Notebook, Sklearn</h3>
<p class="noindent">The Python ecosystem is unique because of the history of how it all came together. I remember first hearing about Python in 2000 when I was working at Caltech. At the time, it was an almost unheard-of language but was gaining some ground in academic circles because there was talk about how great a language it would be for teaching fundamental computer science.</p>
<p class="noindent">One of the issues with teaching computer science with C or Java is that there is a lot of extra overhead to worry about other than fundamental concepts like a <code class="calibre11">for</code> loop. Almost 20 years later we are finally there, and Python is perhaps becoming the standard to teach computer science concepts the world over.</p>
<p class="noindent"><span epub:type="pagebreak" id="page_28"></span>It is no surprise that it has also made tremendous strides in three areas I am also passionate about: DevOps, cloud architecture, and of course data science. For me it is natural to think about all of topics as interrelated. This accidental evolution has made me smile when I realize I can contribute to just about the entire stack of a company with just one language; what is more, I can do it very, very quickly with tremendous scale characteristics because of services like AWS Lambda.</p>
<p class="noindent">Jupyter Notebook is also an interesting beast. I coauthored a book around 10 years ago in which we used IPython for the majority of the book examples (much like this book), and I have literally never stopped using IPython. So when Jupyter became a big deal, it fit just like a glove. The accidents have kept evolving in my favor with Python.</p>
<p class="noindent">As I get more analytical, though, I did spend years working with R and came to appreciate that it has very different style from Python. For one, data frames are embedded into the language as well as plotting and advanced statistical functions and more “pure” functional programming.</p>
<p class="noindent">In comparison to R, Python has tremendously more real-world applicability considering the cloud integration options and libraries, but for “pure” data science it does have a slightly disjointed feel to it. For example, you would never use a <code class="calibre11">for</code> loop in Pandas, but in regular Python it is very common. There are some paradigm clashes as regular Python mixes with Pandas, scikit-learn, and NumPy, but this is a pretty good problem to have if you’re a Python user.</p>
<h3 id="ch02lev2" class="calibre12">R, RStudio, Shiny, and ggplot</h3>
<p class="noindent">Even if the reader is only a Python user, I still think it is useful to have some familiarity with R and the R tools. The R ecosystem has some features that are worth discussing. You can download R from a mirror at <a href="https://https://cran.r-project.org/mirrors.html" class="calibre7">cran.r-project.org/mirrors.html</a>.</p>
<p class="noindent">The predominant integrated development environment (IDE) for the R language is RStudio (<a href="https://www.rstudio.com/" class="calibre7">https://www.rstudio.com/</a>). RStudio has many great features, including the ability to create Shiny (<a href="http://shiny.rstudio.com/" class="calibre7">http://shiny.rstudio.com/</a>) applications and R Markdown documents (<a href="https://rmarkdown.rstudio.com/" class="calibre7">https://rmarkdown.rstudio.com/</a>). If you are just getting started with R, it is almost mandatory to at least give it a try.</p>
<p class="noindent">Shiny is a technology that is evolving but is worth exploring for interactive data analysis. It allows for the creation of interactive web applications that can be deployed to production by writing pure R code. There are many examples in the gallery (<a href="https://rmarkdown.rstudio.com/gallery.html" class="calibre7">https://rmarkdown.rstudio.com/gallery.html</a>) to provide inspiration.</p>
<p class="noindent">Another area in which R is strong is having cutting-edge statistical libraries. Because of the history of R, created in Auckland, New Zealand for statisticians, it has enjoyed a strong history in that community. For that reason alone, it is recommended to have R in your toolbox.</p>
<p class="noindent">Finally, the graphing library ggplot is so good and so complete that in many cases I find myself exporting code from a Python project as a CSV file, bringing it into RStudio, and creating a beautiful visualization in R and ggplot. An example of this is covered in <a href="part0017.html#ch06" class="calibre7">Chapter 6</a>, “<a href="part0017.html#ch06" class="calibre7">Predicting Social-Media Influence in the NBA</a>.”</p>
<h3 id="ch02lev3" class="calibre12">Spreadsheets: Excel and Google Sheets</h3>
<p class="noindent">Excel has gotten a lot of criticism in recent years. But as tools that are not the entire solution, Excel and Google Sheets have been extremely helpful. In particular, one of the powerful ways to <span epub:type="pagebreak" id="page_29"></span>use Excel is as a way of preparing and cleaning data. During the process of rapidly prototyping a data science problem, it can be faster to clean up and format data sets in Excel.</p>
<p class="noindent">Similarly, Google Sheets is wonderful technology that solves real-world problems. In <a href="part0014.html#ch04" class="calibre7">Chapter 4</a>, “<a href="part0014.html#ch04" class="calibre7">Cloud AI Development with Google Cloud Platform,</a>” an example shows how easy it is to programmatically create Google Sheets. Spreadsheets may be legacy technology, but they are still incredibly useful tools to create production solutions.</p>
<h3 id="ch02lev4" class="calibre12">Cloud AI Development with Amazon Web Services</h3>
<p class="noindent">Amazon Web Services (AWS) is the 800-pound gorilla of the cloud. At Amazon, there are about a baker’s dozen leadership principles (<a href="https://https://www.amazon.jobs/principles" class="calibre7">www.amazon.jobs/principles</a>) outlining the way the employees think as an organization. Near the end of the list is “Deliver results.” The cloud platform has been doing just that since it launched in 2006. Prices continuously drop, existing services get better, and new services are added at a rapid pace.</p>
<p class="noindent">In recent years, there has been huge progress in Big Data, AI, and ML on AWS. There has also been a move in the direction of serverless technologies, like AWS Lambda. With many new technologies, there is an initial version that has a link to the past, and then a next iteration that leaves much of the past behind. The first version of the cloud has definite roots in the data center: virtual machines, relational databases, etc. The next iteration, serverless, is “cloud-native” technology. The operating system and other details are abstracted away, and what is left is code to solve the problem.</p>
<p class="noindent">This streamlined way to write code is a natural fit for ML and AI applications being developed on the cloud. This chapter covers these new technologies as they relate to building cloud AI applications.</p>
<h3 id="ch02lev5" class="calibre12">DevOps on AWS</h3>
<p class="noindent">I have heard many data scientists and developers say, “DevOps is not my job.” DevOps isn’t a job, however; it is a state of mind. The most sophisticated forms of AI are automating difficult human tasks like driving cars. DevOps has common roots with this way of thinking. Why wouldn’t an ML engineer want to create a highly efficient feedback loop for deploying software to production?</p>
<p class="noindent">The cloud—and in particular, the AWS cloud—has enabled automation and efficiency at a scale that has never been seen before. Some of the solutions available that are DevOps heavy are Spot instances, OpsWorks, Elastic Beanstalk, Lambda, CodeStar, CodeCommit, CodeBuild, CodePipeline, and CodeDeploy. This chapter shows examples of all these services along with an overview on how an ML engineer can make use of them.</p>
<h4 id="ch02lev5sub1" class="calibre16">Continuous Delivery</h4>
<p class="noindent">Software is always ready to be released in a continuous-delivery environment. The conceptual model is a factory assembly line. There is a great online resource, DevOps Essentials on AWS (<a href="http://www.devopsessentialsaws.com/" class="calibre7">http://www.devopsessentialsaws.com/</a>), codeveloped by Pearson Education and Stelligent, that covers much of this. It gives a good overview of DevOps on AWS.</p>
<h4 id="ch02lev5sub2" class="calibre16"><span epub:type="pagebreak" id="page_30" class="calibre2"></span>Creating a Software Development Environment for AWS</h4>
<p class="noindent">An easy-to-miss detail in working with AWS is getting a basic development environment set up. Because Python is such a large portion of this setup for many ML developers, it makes sense to design around Python. This involves setting up a Makefile, creating a virtual environment, adding a shortcut in bash or zsh to switch into it, automatically sourcing an AWS profile, and more.</p>
<p class="noindent">A quick note on Makefiles: They first appeared in 1976 from Bell Labs and are used as a dependency-tracking build utility. Makefile systems can get very complex, but for many ML projects they can be invaluable. One good reason is they are available on any Unix or Linux system without installing software. Second, they are a standard that, generally speaking, people on a project will understand.</p>
<p class="noindent">Virtual environments were something I used quite a bit while working in the film industry. Film could be considered one of the first “Big Data” industries. Even in the late 2000s there were near, or actual, petabyte file servers at film studios where I worked, and the only way to get a grip on the directory trees was to have tools that set environmental variables for projects.</p>
<p class="noindent">When I was working for Weta Digital doing Python programming for the movie <em class="calibre5">Avatar,</em> I remember the file servers got so big they couldn’t grow anymore so they had to sync huge copies of the data around to several file servers. In fact, one of my side projects was to help fix the way Python worked on the file servers. Because the Python import system was so greedy in the way it looked for imports, often a script would take 30 seconds to launch because it was searching nearly a hundred thousand files in its path. We discovered this by using strace on Linux, then hacking the Python interpreter to ignore Python paths.</p>
<p class="noindent">Virtualenv in Python and conda by Anaconda do similar things to what I experienced in the film world. They create isolated environmental variables for a project. This then allows a user to toggle among the projects they work on and not get conflicting versions of libraries.</p>
<p class="noindent">In <a href="part0011.html#ch2list1" class="calibre7">Listing 2.1</a>, there is the start of a basic Makefile.</p>
<div class="side-exe">
<p class="ex-caption"><a id="ch2list1" class="calibre20"></a><span class="calibre3">Listing 2.1</span> <strong class="calibre3">Basic Python AWS Project Makefile</strong></p>
<p class="codelink"><a id="p030pro01" href="part0031_split_000.html#p030pro01a" class="calibre7">Click here to view code image</a></p>
<p class="hr"></p>
<p class="pre-ex">setup:<br class="calibre9"/>
    python3 -m venv ~/.pragai-aws<br class="calibre9"/>
install:<br class="calibre9"/>
    pip install -r requirements.txt</p>
</div>
<p class="noindent">A Makefile is best practice because it serves as a common reference point for building a project locally, on a build server, in a docker container, and in production. To use this Makefile in a new git repository type:</p>
<p class="codelink"><a id="p030pro02" href="part0031_split_001.html#p030pro02a" class="calibre7">Click here to view code image</a></p>
<p class="pre">→   pragai-aws git:(master) ✗  make setup<br class="calibre9"/>
python3 -m venv ~/.pragai-aws</p>
<p class="noindent">This make command creates a new Python virtual environment in the <code class="calibre11">~/.pragai-aws</code> location. As mentioned other places in this book, it is generally a good idea to create an alias that will <code class="calibre11">source</code> the environment and cd into it all at once. For a Z shell or bash user, you would edit the .zshrc or .bashrc and add an alias to this git checkout repository.</p>
<p class="codelink"><a id="p031pro01" href="part0031_split_002.html#p031pro01a" class="calibre7">Click here to view code image</a></p>
<p class="pre"><span epub:type="pagebreak" id="page_31"></span>Alias pawstop="cd ~/src/pragai-aws &amp;&amp;\<br class="calibre9"/>
         source ~/.pragai-aws/bin/activate"</p>
<p class="noindent">Then sourcing this environment is as easy as typing this alias.</p>
<p class="codelink"><a id="p031pro02" href="part0031_split_003.html#p031pro02a" class="calibre7">Click here to view code image</a></p>
<p class="pre">→   pragai-aws git:(master) ✗  pawstop<br class="calibre9"/>
(.pragai-aws) →   pragai-aws git:(master) ✗</p>
<p class="noindent">The magic that makes this happen is the activate script. This same activate script will serve as a useful mechanism to control other environmental variables for the project, namely <code class="calibre11">PYTHONPATH</code> and <code class="calibre11">AWS_PROFILE</code>, which will be covered in detail. The next step to set up a project for AWS is to create an account if you don’t have one, and a user in that account if you haven’t already. Amazon has excellent instructions on creating Identity and Access Management (IAM) user accounts: <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html" class="calibre7">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html.</a></p>
<p class="noindent">Once the account is set up (following the official AWS instructions), the next setup step is to create a named profile. There is good official reference material from AWS on this as well: <a href="http://docs.aws.amazon.com/cli/latest/userguide/cli-multiple-profiles.html" class="calibre7">http://docs.aws.amazon.com/cli/latest/userguide/cli-multiple-profiles.html</a>. The key idea is to create profiles that make it explicit that a project is using a particular profile username or role; refer to AWS materials for more detail: <a href="http://docs.aws.amazon.com/cli/latest/userguide/cli-roles.html" class="calibre7">http://docs.aws.amazon.com/cli/latest/userguide/cli-roles.html</a>.</p>
<p class="noindent">To install the AWS CLI tool and Boto3 (note: Boto3 is the latest version of Boto as of this writing), place both in the requirements.txt file and run <code class="calibre11">make install</code>.</p>
<p class="codelink"><a id="p031pro03" href="part0031_split_004.html#p031pro03a" class="calibre7">Click here to view code image</a></p>
<p class="pre">(.pragai-aws) →   ✗  make install<br class="calibre9"/>
pip install -r requirements.txt</p>
<p class="noindent">With the <code class="calibre11">aws</code> command installed, it needs to be configured with a new user.</p>
<p class="codelink"><a id="p031pro04" href="part0031_split_005.html#p031pro04a" class="calibre7">Click here to view code image</a></p>
<p class="pre">(.pragai-aws) →   ✗  aws configure --profile pragai<br class="calibre9"/>
AWS Access Key ID [****************XQDA]:<br class="calibre9"/>
AWS Secret Access Key [****************nmkH]:<br class="calibre9"/>
Default region name [us-west-2]:<br class="calibre9"/>
Default output format [json]:</p>
<p class="noindent">The command-line <code class="calibre11">aws</code> tool can now be used with the <code class="calibre11">–profile</code> option. An easy way to  test this out is to attempt to list the contents of one of the ML data sets hosted on AWS, like the Global Database of Events, Language and Tone Project (GDELT; <a href="https://aws.amazon.com/public-datasets/gdelt/" class="calibre7">https://aws.amazon.com/public-datasets/gdelt/</a>).</p>
<p class="codelink"><a id="p031pro05" href="part0031_split_006.html#p031pro05a" class="calibre7">Click here to view code image</a></p>
<p class="pre">(.pragai-aws) → aws s3 cp\<br class="calibre9"/>
         s3://gdelt-open-data/events/1979.csv .<br class="calibre9"/>
fatal error: Unable to locate credentials</p>
<p class="noindent">By selecting the <code class="calibre11">–profile</code> option, the download command is able to grab the file from S3. There is certainly no problem with always remembering to type the <code class="calibre11">–profile</code> flag, but this could get tedious for heavy command-line use of AWS.</p>
<p class="codelink"><a id="p031pro06" href="part0031_split_007.html#p031pro06a" class="calibre7">Click here to view code image</a></p>
<p class="pre">(.pragai-aws) aws --profile pragai s3 cp\<br class="calibre9"/>
         s3://gdelt-open-data/events/1979.csv .<br class="calibre9"/>
download: s3://gdelt-open-data/events/1979.csv to ./1979.csv<br class="calibre9"/>
(.pragai-aws) →   du -sh 1979.csv<br class="calibre9"/>
110M    1979.csv</p>
<p class="noindent"><span epub:type="pagebreak" id="page_32"></span>A solution to this is to put the <code class="calibre11">AWS_PROFILE</code> variable in the activate script for virtualenv.</p>
<p class="codelink"><a id="p032pro01" href="part0031_split_008.html#p032pro01a" class="calibre7">Click here to view code image</a></p>
<p class="pre">(.pragia-aws) →  vim ~/.pragai-aws/bin/activate<br class="calibre9"/>
#Export AWS Profile<br class="calibre9"/>
AWS_PROFILE="pragai"<br class="calibre9"/>
export AWS_PROFILE</p>
<p class="noindent">Now when the virtual environment is sourced, the correct AWS profile is automatically used.</p>
<p class="codelink"><a id="p032pro02" href="part0031_split_009.html#p032pro02a" class="calibre7">Click here to view code image</a></p>
<p class="pre">(.pragia-aws) →  echo $AWS_PROFILE<br class="calibre9"/>
pragai<br class="calibre9"/>
(.pragia-aws) →  aws s3 cp\<br class="calibre9"/>
         s3://gdelt-open-data/events/1979.csv .<br class="calibre9"/>
download: s3://gdelt-open-data/events/1979.csv to ./1979.csv  </p>
<h5 class="calibre17">Python AWS Project Configuration</h5>
<p class="noindent">With the virtualenv and AWS credentials set up, the next thing to configure is the Python code. Having a valid project structure goes a long way in making development efficient and productive. Here is an example of creating a basic Python/AWS project structure.</p>
<p class="codelink"><a id="p032pro03" href="part0031_split_010.html#p032pro03a" class="calibre7">Click here to view code image</a></p>
<p class="pre">(.pragia-aws) →  mkdir paws<br class="calibre9"/>
(.pragia-aws) →  touch paws/__init__.py<br class="calibre9"/>
(.pragia-aws) →  touch paws/s3.py<br class="calibre9"/>
(.pragia-aws) →  mkdir tests<br class="calibre9"/>
(.pragia-aws) →  touch tests/test_s3.py</p>
<p class="noindent">Next, a simple S3 module can be written that uses this layout. An example is seen in <a href="part0011.html#ch2list2" class="calibre7">Listing 2.2</a>. The Boto3 library is used to create a function to download a file from S3. The additional import is a logging library.</p>
<div class="side-exe">
<p class="ex-caption"><a id="ch2list2" class="calibre20"></a><span class="calibre3">Listing 2.2</span> <strong class="calibre3">S3 Module</strong></p>
<p class="codelink"><a id="p032pro04" href="part0031_split_011.html#p032pro04a" class="calibre7">Click here to view code image</a></p>
<p class="hr"></p>
<p class="pre-ex">"""<br class="calibre9"/>
S3 methods for PAWS library<br class="calibre9"/>
"""<br class="calibre9"/>
<br class="calibre9"/>
import boto3<br class="calibre9"/>
from sensible.loginit import logger<br class="calibre9"/>
<br class="calibre9"/>
log = logger("Paws")<br class="calibre9"/>
<br class="calibre9"/>
<br class="calibre9"/>
def boto_s3_resource():<br class="calibre9"/>
    """Create boto3 S3 Resource"""<br class="calibre9"/>
<br class="calibre9"/>
    return boto3.resource("s3")<br class="calibre9"/>
<br class="calibre9"/>
def download(bucket, key, filename, resource=None):<br class="calibre9"/>
    """Downloads file from s3"""<br class="calibre9"/>
<br class="calibre9"/>
    if resource is None:<br class="calibre9"/>
        resource = boto_s3_resource()<br class="calibre9"/>
    log_msg = "Attempting download: {bucket}, {key}, {filename}".\<br class="calibre9"/>
        format(bucket=bucket, key=key, filename=filename)<br class="calibre9"/>
    log.info(log_msg)<br class="calibre9"/>
    resource.meta.client.download_file(bucket, key, filename)<br class="calibre9"/>
    return filename</p>
</div>
<p class="noindent"><span epub:type="pagebreak" id="page_33"></span>To use this newly created library from the IPython command line, it is a couple of lines of code, and the namespace of “paws” has been created.</p>
<p class="codelink"><a id="p033pro01" href="part0031_split_012.html#p033pro01a" class="calibre7">Click here to view code image</a></p>
<p class="pre">In [1]: from paws.s3 import download<br class="calibre9"/>
<br class="calibre9"/>
In [2]: download(bucket="gdelt-open-data",\<br class="calibre9"/>
         key="events/1979.csv", filename="1979.csv")<br class="calibre9"/>
2017-09-02 11:28:57,532 - Paws - INFO - Attempting download:<br class="calibre9"/>
         gdelt-open-data, events/1979.csv, 1979.csv</p>
<p class="noindent">With a successful library started, it is time to create a <code class="calibre11">PYTHONPATH</code> variable in the activate script that reflects the location of this library.</p>
<p class="pre">#Export PYTHONPATH<br class="calibre9"/>
PYTHONPATH="paws"<br class="calibre9"/>
export PYTHONPATH</p>
<p class="noindent">Next, the virtual environment is sourced again using the alias pawstop set up earlier.</p>
<p class="codelink"><a id="p033pro02" href="part0031_split_013.html#p033pro02a" class="calibre7">Click here to view code image</a></p>
<p class="pre">(.pragia-aws) →  pawstop<br class="calibre9"/>
(.pragia-aws) →  echo $PYTHONPATH<br class="calibre9"/>
paws</p>
<p class="noindent">Next, a unit test can be created using <em class="calibre5">pytest</em> and <em class="calibre5">moto</em>, two useful libraries for testing AWS. Moto is used for mocking AWS and pytest is a test framework. This can be seen in <a href="part0011.html#ch2list3" class="calibre7">Listing 2.3</a>. Pytest fixtures are used to create temporary resources, and then moto is used to create mock objects that simulate Boto actions. The test function <code class="calibre11">test_download</code> then asserts that after the resource is created properly. Note that to actually test the download function, a resource object must be passed in. This is a great example of how writing tests for code helps make code less brittle.</p>
<div class="side-exe">
<p class="ex-caption"><a id="ch2list3" class="calibre20"></a><span class="calibre3">Listing 2.3</span> <strong class="calibre3">Testing S3 Module</strong></p>
<p class="codelink"><a id="p033pro03" href="part0031_split_014.html#p033pro03a" class="calibre7">Click here to view code image</a></p>
<p class="hr"></p>
<p class="pre-ex">import pytest<br class="calibre9"/>
import boto3<br class="calibre9"/>
from moto import mock_s3<br class="calibre9"/>
from paws.s3 import download<br class="calibre9"/>
<br class="calibre9"/>
@pytest.yield_fixture(scope="session")<br class="calibre9"/>
def mock_boto():<br class="calibre9"/>
    """Setup Mock Objects"""<br class="calibre9"/>
    <br class="calibre9"/>
    mock_s3().start()<br class="calibre9"/>
    output_str = 'Something'<br class="calibre9"/>
    resource = boto3.resource('s3')<br class="calibre9"/>
    resource.create_bucket(Bucket="gdelt-open-data")<br class="calibre9"/>
    resource.Bucket("gdelt-open-data").\<br class="calibre9"/>
        put_object(Key="events/1979.csv",<br class="calibre9"/>
                        Body=output_str)<br class="calibre9"/>
    yield resource<br class="calibre9"/>
    mock_s3().stop()<br class="calibre9"/>
<br class="calibre9"/>
def test_download(mock_boto):<br class="calibre9"/>
    """Test s3 download function"""<br class="calibre9"/>
<br class="calibre9"/>
    resource = mock_boto<br class="calibre9"/>
    res = download(resource=resource, bucket="gdelt-open-data",<br class="calibre9"/>
                key="events/1979.csv",filename="1979.csv")<br class="calibre9"/>
    assert res == "1979.csv"</p>
</div>
<p class="noindent"><span epub:type="pagebreak" id="page_34"></span>To install the libraries needed to test the project, the contents of the requirements.txt file should look like this.</p>
<p class="pre">awscli<br class="calibre9"/>
boto3<br class="calibre9"/>
moto<br class="calibre9"/>
pytest<br class="calibre9"/>
pylint<br class="calibre9"/>
sensible<br class="calibre9"/>
jupyter<br class="calibre9"/>
pytest-cov<br class="calibre9"/>
pandas</p>
<p class="noindent">To install the packages, a <code class="calibre11">make install</code> command is run. Then to run the tests, the Makefile should be changed to look like this.</p>
<p class="codelink"><a id="p034pro01" href="part0031_split_015.html#p034pro01a" class="calibre7">Click here to view code image</a></p>
<p class="pre">setup:<br class="calibre9"/>
    python3 -m venv ~/.pragia-aws<br class="calibre9"/>
<br class="calibre9"/>
install:<br class="calibre9"/>
    pip install -r requirements.txt<br class="calibre9"/>
<br class="calibre9"/>
test:<br class="calibre9"/>
    PYTHONPATH=. &amp;&amp; pytest -vv --cov=paws tests/*.py<br class="calibre9"/>
<br class="calibre9"/>
lint:<br class="calibre9"/>
    pylint --disable=R,C paws</p>
<p class="noindent">Then, running the tests and the coverage looks like this.</p>
<p class="codelink"><a id="p034pro01zz" href="part0031_split_016.html#p034pro01z" class="calibre7">Click here to view code image</a></p>
<p class="pre">(.pragia-aws) →  pragai-aws git:(master) ✗ make test<br class="calibre9"/>
PYTHONPATH=. &amp;&amp; pytest -vv --cov=paws tests/*.py<br class="calibre9"/>
=================================================<br class="calibre9"/>
test session starts ==============================<br class="calibre9"/>
platform darwin -- Python 3.6.2, pytest-3.2.1,<br class="calibre9"/>
/Users/noahgift/.pragia-aws/bin/python3<br class="calibre9"/>
cachedir: .cache<br class="calibre9"/>
rootdir: /Users/noahgift/src/pragai-aws, inifile:<br class="calibre9"/>
plugins: cov-2.5.1<br class="calibre9"/>
collected 1 item<br class="calibre9"/>
<br class="calibre9"/>
tests/test_s3.py::test_download PASSED<br class="calibre9"/>
<br class="calibre9"/>
---------- coverage: platform darwin, python 3.6.2-final-0<br class="calibre9"/>
Name               Stmts   Miss  Cover<br class="calibre9"/>
--------------------------------------<br class="calibre9"/>
paws/__init__.py       0      0   100%<br class="calibre9"/>
paws/s3.py            12      2    83%<br class="calibre9"/>
--------------------------------------<br class="calibre9"/>
TOTAL                 12      2    83%</p>
<p class="noindent"><span epub:type="pagebreak" id="page_35"></span>There are many ways to set up Pylint, but my preferred method is to only show warnings and errors for a continuous delivery project:  <code class="calibre11">pylint --disable=R,C paws</code>. Running the lint, then, works like this.</p>
<p class="codelink"><a id="p035pro01" href="part0031_split_017.html#p035pro01a" class="calibre7">Click here to view code image</a></p>
<p class="pre">(.pragia-aws) →  pragai-aws git:(master) ✗ make lint<br class="calibre9"/>
pylint --disable=R,C paws<br class="calibre9"/>
No config file found, using default configuration<br class="calibre9"/>
<br class="calibre9"/>
--------------------------------------------------------------------<br class="calibre9"/>
Your code has been rated at 10.00/10 (previous run: 10.00/10, +0.00)</p>
<p class="noindent">Finally, it may be useful to create an all statement to install, lint, and test:  <code class="calibre11">all: install lint test</code>. Then a <code class="calibre11">make all</code> command will start all three actions sequentially.</p>
<h4 id="ch02lev5sub3" class="calibre16">Integrating Jupyter Notebook</h4>
<p class="noindent">Getting Jupyter Notebook to work with the project layout and having notebooks automatically tested are great benefits to add. To do that, a notebook will be created in a notebooks folder and a data folder at the root of the checkout:  <code class="calibre11">mkdir -p notebooks</code>. Next, <code class="calibre11">jupyter notebook</code> is run and a new notebook is created called <code class="calibre11">paws.ipynb</code>. Inside this notebook, the previous library can be used to download the CSV file and do a brief exploration in Pandas. First, the path is appended to the root and Pandas is imported.</p>
<p class="codelink"><a id="p035pro02" href="part0031_split_018.html#p035pro02a" class="calibre7">Click here to view code image</a></p>
<p class="pre">In [1]: #Add root checkout to path<br class="calibre9"/>
   ...: import sys<br class="calibre9"/>
   ...: sys.path.append("..")<br class="calibre9"/>
   ...: import pandas as pd<br class="calibre9"/>
   ...:</p>
<p class="noindent">The library that was created earlier is loaded, and the CSV file is downloaded.</p>
<p class="codelink"><a id="p035pro02zz" href="part0031_split_019.html#p035pro02z" class="calibre7">Click here to view code image</a></p>
<p class="pre">In [2]: from paws.s3 import (boto_s3_resource, download)<br class="calibre9"/>
<br class="calibre9"/>
In [3]: resource = boto_s3_resource()<br class="calibre9"/>
<br class="calibre9"/>
In [4]: csv_file = download(resource=resource,<br class="calibre9"/>
   ...:                     bucket="gdelt-open-data",<br class="calibre9"/>
   ...:                     key="events/1979.csv",<br class="calibre9"/>
   ...:                     filename="1979.csv")<br class="calibre9"/>
   ...:                    <br class="calibre9"/>
2017-09-03 11:57:35,162 - Paws - INFO - Attempting<br class="calibre9"/>
events/1979.csv, 1979.csv</p>
<p class="noindent"><span epub:type="pagebreak" id="page_36"></span>Because the data is irregularly shaped, a trick to fit it into the data frame is used: <code class="calibre11">names=range(5)</code>. Also, at 100MB, the file is too big to fit into a Git repo as a test data set. It will be truncated and saved back out.</p>
<p class="codelink"><a id="p036pro01" href="part0031_split_020.html#p036pro01a" class="calibre7">Click here to view code image</a></p>
<p class="pre">In [7]: #Load the file, truncate it and save.<br class="calibre9"/>
   ...: df = pd.read_csv(csv_file, names=range(5))<br class="calibre9"/>
   ...: df = df.head(100)<br class="calibre9"/>
   ...: df.to_csv(csv_file)<br class="calibre9"/>
   ...:</p>
<p class="noindent">Next the file is read back in and described.</p>
<p class="codelink"><a id="p036pro02" href="part0031_split_021.html#p036pro02a" class="calibre7">Click here to view code image</a></p>
<p class="pre">In [8]: df = pd.read_csv("1979.csv", names=range(5))<br class="calibre9"/>
   ...: df.head()<br class="calibre9"/>
   ...:<br class="calibre9"/>
Out[8]:<br class="calibre9"/>
   Unnamed: 0                                                  <br class="calibre9"/>
0         NaN                                                    <br class="calibre9"/>
1         NaN                                                    <br class="calibre9"/>
2         0.0  0\t19790101\t197901\t1979\t1979.0027\t\t\t\t\t...    <br class="calibre9"/>
3         1.0  1\t19790101\t197901\t1979\t1979.0027\t\t\t\t\t...  <br class="calibre9"/>
4         2.0  2\t19790101\t197901\t1979\t1979.0027\t\t\t\t\t...<br class="calibre9"/>
<br class="calibre9"/>
     3    4  <br class="calibre9"/>
0    3    4  <br class="calibre9"/>
1    3    4  <br class="calibre9"/>
2  NaN  NaN  <br class="calibre9"/>
3  NaN  NaN  <br class="calibre9"/>
4  NaN  NaN  <br class="calibre9"/>
<br class="calibre9"/>
In [9]: df.describe()<br class="calibre9"/>
Out[9]:<br class="calibre9"/>
       Unnamed: 0<br class="calibre9"/>
count   98.000000<br class="calibre9"/>
mean    48.500000<br class="calibre9"/>
std     28.434134<br class="calibre9"/>
min      0.000000<br class="calibre9"/>
25%     24.250000<br class="calibre9"/>
50%     48.500000<br class="calibre9"/>
75%     72.750000<br class="calibre9"/>
max     97.000000</p>
<p class="noindent"><span epub:type="pagebreak" id="page_37"></span>With this basic notebook set up, it can also be integrated into the Makefile build system by using the pytest plugin nbval and adding it to the requirements.txt file. The lines below should be commented out (so the S3 file won’t be downloaded on each run, and then the notebook can be saved and closed).</p>
<p class="codelink"><a id="p037pro01" href="part0031_split_022.html#p037pro01a" class="calibre7">Click here to view code image</a></p>
<p class="pre">#csv_file = download(resource=resource,<br class="calibre9"/>
#                    bucket="gdelt-open-data",<br class="calibre9"/>
#                    key="events/1979.csv",<br class="calibre9"/>
#                    filename="1979.csv")<br class="calibre9"/>
#Load the file, truncate it and save.<br class="calibre9"/>
#df = pd.read_csv(csv_file, names=range(5))<br class="calibre9"/>
#df = df.head(100)<br class="calibre9"/>
#df.to_csv(csv_file)</p>
<p class="noindent">In the Makefile, a new line can be added to also test the notebooks.</p>
<p class="codelink"><a id="p037pro02" href="part0031_split_023.html#p037pro02a" class="calibre7">Click here to view code image</a></p>
<p class="pre">test:<br class="calibre9"/>
    PYTHONPATH=. &amp;&amp; pytest -vv --cov=paws tests/*.py<br class="calibre9"/>
    PYTHONPATH=. &amp;&amp; py.test --nbval-lax notebooks/*.ipynb</p>
<p class="noindent">The output of the notebook test run looks like this now.</p>
<p class="codelink"><a id="p037pro02z" href="part0031_split_024.html#p037pro02zz" class="calibre7">Click here to view code image</a></p>
<p class="pre">PYTHONPATH=. &amp;&amp; py.test --nbval-lax notebooks/*.ipynb<br class="calibre9"/>
===================================================================<br class="calibre9"/>
test session starts<br class="calibre9"/>
====================================================================<br class="calibre9"/>
platform darwin -- Python 3.6.2, pytest-3.2.1, py-1.4.34<br class="calibre9"/>
rootdir: /Users/noahgift/src/pragai-aws, inifile:<br class="calibre9"/>
plugins: cov-2.5.1, nbval-0.7<br class="calibre9"/>
collected 8 items<br class="calibre9"/>
<br class="calibre9"/>
notebooks/paws.ipynb ........<br class="calibre9"/>
<br class="calibre9"/>
=======================================================<br class="calibre9"/>
warnings summary ======================================<br class="calibre9"/>
notebooks/paws.ipynb::Cell 0<br class="calibre9"/>
  /Users/noahgift/.pragia-aws/lib/python3.6/site-<br class="calibre9"/>
packages/jupyter_client/connect.py:157: RuntimeWarning:<br class="calibre9"/>
'/var/folders/vl/sskrtrf17nz4nww5zr1b64980000gn/T':<br class="calibre9"/>
'/var/folders/vl/sskrtrf17nz4nww5zr1b64980000gn/T'<br class="calibre9"/>
    RuntimeWarning,<br class="calibre9"/>
<br class="calibre9"/>
-- Docs: http://doc.pytest.org/en/latest/warnings.html<br class="calibre9"/>
========================================================<br class="calibre9"/>
8 passed, 1 warnings in 4.08 seconds<br class="calibre9"/>
========================================================</p>
<p class="noindent">There is a now a repeatable and testable structure for adding notebooks to the project and sharing the common library that has been created. Additionally, this structure can be used for a continuous-delivery environment, which will be shown later in the chapter. By testing the Jupyter Notebooks as they are built, it also serves a useful double purpose of integration testing an ML project.</p>
<h4 id="ch02lev5sub4" class="calibre16"><span epub:type="pagebreak" id="page_38" class="calibre2"></span>Integrating Command-Line Tools</h4>
<p class="noindent">An often-overlooked tool in both traditional software engineering projects and ML projects is the addition of command-line tools. Some interactive exploration is much more conducive to a command-line tool. For cloud architecture, creating a command-line prototype of say, an SQS-based application, is often much quicker than only using a traditional technique like an IDE. To get started with building a command-line tool, the requirements.txt file will be updated with the <strong class="calibre6">Click</strong> library and <code class="calibre11">make install</code> will be run.</p>
<p class="codelink"><a id="p038pro01" href="part0031_split_025.html#p038pro01a" class="calibre7">Click here to view code image</a></p>
<p class="pre">(.pragia-aws) →  tail -n 2 requirements.txt<br class="calibre9"/>
click</p>
<p class="noindent">Next, a command-line script is created in the root.</p>
<p class="codelink"><a id="p038pro02" href="part0031_split_026.html#p038pro02a" class="calibre7">Click here to view code image</a></p>
<p class="pre">(.pragia-aws) →  pragai-aws git:(master) touch pcli.py</p>
<p class="noindent">The script will perform a set of actions similar to those of the Jupyter Notebook, except that it will be more flexible and allow the user to pass in the arguments to the function from the command line. In <a href="part0011.html#ch2list4" class="calibre7">Listing 2.4</a>, the click framework is used to create a wrapper around the Python library created previously.</p>
<div class="side-exe">
<p class="ex-caption"><a id="ch2list4" class="calibre20"></a><span class="calibre3">Listing 2.4</span> <strong class="calibre3">pcli Command-Line Tool</strong></p>
<p class="codelink"><a id="p038pro03" href="part0031_split_027.html#p038pro03a" class="calibre7">Click here to view code image</a></p>
<p class="hr"></p>
<p class="pre-ex">#!/usr/bin/env python<br class="calibre9"/>
<br class="calibre9"/>
"""<br class="calibre9"/>
Command-line Tool for Working with PAWS library<br class="calibre9"/>
"""<br class="calibre9"/>
import sys<br class="calibre9"/>
<br class="calibre9"/>
import click<br class="calibre9"/>
import paws<br class="calibre9"/>
from paws import s3<br class="calibre9"/>
<br class="calibre9"/>
@click.version_option(paws.__version__)<br class="calibre9"/>
@click.group()<br class="calibre9"/>
def cli():<br class="calibre9"/>
    """PAWS Tool"""<br class="calibre9"/>
<br class="calibre9"/>
@cli.command("download")<br class="calibre9"/>
@click.option("--bucket", help="Name of S3 Bucket")<br class="calibre9"/>
@click.option("--key", help="Name of S3 Key")<br class="calibre9"/>
@click.option("--filename", help="Name of file")<br class="calibre9"/>
def download(bucket, key, filename):<br class="calibre9"/>
    """Downloads an S3 file<br class="calibre9"/>
    ./paws-cli.py --bucket gdelt-open-data --key \<br class="calibre9"/>
        events/1979.csv --filename 1979.csv<br class="calibre9"/>
    """<br class="calibre9"/>
<br class="calibre9"/>
    if not bucket and not key and not filename:<br class="calibre9"/>
        click.echo("--bucket and --key and --filename are required")<br class="calibre9"/>
        sys.exit(1)<br class="calibre9"/>
    click.echo(<br class="calibre9"/>
        "Downloading s3 file with: bucket-\<br class="calibre9"/>
        {bucket},key{key},filename{filename}".\<br class="calibre9"/>
        format(bucket=bucket, key=key, filename=filename))<br class="calibre9"/>
    res = s3.download(bucket, key,filename)<br class="calibre9"/>
    click.echo(res)<br class="calibre9"/>
<br class="calibre9"/>
if __name__ == "__main__":<br class="calibre9"/>
    cli()</p>
</div>
<p class="noindent"><span epub:type="pagebreak" id="page_39"></span>To get this script to be executable, it needs to have a Python shebang line added to the top of the script.</p>
<p class="pre">#!/usr/bin/env python</p>
<p class="noindent">It also needs to be made executable, as follows.</p>
<p class="codelink"><a id="p039pro01" href="part0031_split_029.html#p039pro01a" class="calibre7">Click here to view code image</a></p>
<p class="pre">(.pragia-aws) →  pragai-aws git:(master) chmod +x pcli.py</p>
<p class="noindent">Finally, a good trick is to create a __version__ variable inside of the __init__.py portion  of a library and set it to a version number that is a string. It can then be called in a script or a command-line tool.</p>
<p class="noindent">To get this script to run help, the following commands are run.</p>
<p class="codelink"><a id="p039pro02" href="part0031_split_030.html#p039pro02a" class="calibre7">Click here to view code image</a></p>
<p class="pre">(.pragia-aws) →  ./pcli.py --help<br class="calibre9"/>
Usage: paws-cli.py [OPTIONS] COMMAND [ARGS]...<br class="calibre9"/>
<br class="calibre9"/>
  PAWS Tool<br class="calibre9"/>
<br class="calibre9"/>
Options:<br class="calibre9"/>
  --version  Show the version and exit.<br class="calibre9"/>
  --help     Show this message and exit.<br class="calibre9"/>
<br class="calibre9"/>
Commands:<br class="calibre9"/>
  download  Downloads an S3 file ./pcli.py --bucket...</p>
<p class="noindent">To get the script to run the download, the commands and output are</p>
<p class="codelink"><a id="p039pro03" href="part0031_split_031.html#p039pro03a" class="calibre7">Click here to view code image</a></p>
<p class="pre">(.pragia-aws) →  ./pcli.py download –bucket\<br class="calibre9"/>
         gdelt-open-data --key events/1979.csv \<br class="calibre9"/>
         --filename 1979.csv<br class="calibre9"/>
<br class="calibre9"/>
Downloading s3 file with: bucket-gdelt-open-data,<br class="calibre9"/>
keyevents/1979.csv,filename1979.csv<br class="calibre9"/>
2017-09-03 14:55:39,627 - Paws - INFO - Attempting download:<br class="calibre9"/>
 gdelt-open-data, events/1979.csv, 1979.csv<br class="calibre9"/>
1979.csv</p>
<p class="noindent"><span epub:type="pagebreak" id="page_40"></span>This is all it takes to start adding powerful command-line tools to an ML project. One last step is to integrate this into the test infrastructure. Fortunately, <strong class="calibre6">Click has great support for testing as well</strong> (<a href="http://click.pocoo.org/5/testing/" class="calibre7">http://click.pocoo.org/5/testing/</a>). A new file is created with this command: <code class="calibre11">touch tests/test_paws_cli.py</code> In <a href="part0011.html#ch2list5" class="calibre7">Listing 2.5</a>, the test code is written that verifies the <code class="calibre11">__version__ </code>variable flows through the command-line tool.</p>
<div class="side-exe">
<p class="ex-caption"><a id="ch2list5" class="calibre20"></a><span class="calibre3">Listing 2.5</span> Click Command-line Testing of pcli</p>
<p class="codelink"><a id="p040pro01" href="part0031_split_032.html#p040pro01a" class="calibre7">Click here to view code image</a></p>
<p class="hr"></p>
<p class="pre-ex">import pytest<br class="calibre9"/>
import click<br class="calibre9"/>
from click.testing import CliRunner<br class="calibre9"/>
<br class="calibre9"/>
from pcli import cli<br class="calibre9"/>
from paws import __version__<br class="calibre9"/>
<br class="calibre9"/>
@pytest.fixture<br class="calibre9"/>
def runner():<br class="calibre9"/>
    cli_runner = CliRunner()<br class="calibre9"/>
    yield cli_runner<br class="calibre9"/>
<br class="calibre9"/>
<br class="calibre9"/>
def test_cli(runner):<br class="calibre9"/>
    result = runner.invoke(cli, ['--version'])<br class="calibre9"/>
    assert __version__ in result.output  </p>
</div>
<p class="noindent">The Makefile needs to be altered to allow coverage reporting to show for the newly created command-line tool. This is shown below in the output of the make command <code class="calibre11">make test</code>. By adding <code class="calibre11">–cov=pli</code>, everything just works, and the code coverage is computed.</p>
<p class="codelink"><a id="p040pro02" href="part0031_split_033.html#p040pro02a" class="calibre7">Click here to view code image</a></p>
<p class="pre">(.pragia-aws) →  make test<br class="calibre9"/>
PYTHONPATH=. &amp;&amp; pytest -vv --cov=paws --cov=pcli tests/*.py<br class="calibre9"/>
===========================================================<br class="calibre9"/>
test session starts<br class="calibre9"/>
===========================================================<br class="calibre9"/>
platform darwin -- Python 3.6.2, pytest-3.2.1, py-1.4.34,<br class="calibre9"/>
/Users/noahgift/.pragia-aws/bin/python3<br class="calibre9"/>
cachedir: .cache<br class="calibre9"/>
rootdir: /Users/noahgift/src/pragai-aws, inifile:<br class="calibre9"/>
plugins: cov-2.5.1, nbval-0.7<br class="calibre9"/>
collected 2 items<br class="calibre9"/>
<br class="calibre9"/>
tests/test_paws_cli.py::test_cli PASSED<br class="calibre9"/>
tests/test_s3.py::test_download PASSED<br class="calibre9"/>
<br class="calibre9"/>
---------- coverage: platform darwin, python 3.6.2-final-0<br class="calibre9"/>
Name               Stmts   Miss  Cover<br class="calibre9"/>
--------------------------------------<br class="calibre9"/>
paws/__init__.py       1      0   100%<br class="calibre9"/>
paws/s3.py            12      2    83%<br class="calibre9"/>
pcli.py               19      7    63%<br class="calibre9"/>
--------------------------------------<br class="calibre9"/>
TOTAL                 32      9    72%</p>
<h4 id="ch02lev5sub5" class="calibre16"><span epub:type="pagebreak" id="page_41" class="calibre2"></span>Integrating AWS CodePipeline</h4>
<p class="noindent">A rich project skeleton for working on AWS projects is working, tested, and building. A next step is to integrate the AWS CodePipeline toolchain. AWS CodePipeline is a very powerful collection of tools on AWS that acts like a Swiss Army Knife of continuous delivery. It has the flexibility to be extended in many different directions. In this example, a basic build server configuration will be set up that triggers on changes to GitHub. First, a new file <code class="calibre11">touch buildspec.yml</code> is created; then, it is populated with the same make commands that are run locally, as seen in <a href="part0011.html#ch2list6" class="calibre7">Listing 2.6</a>.</p>
<p class="noindent">To get this building, a new CodePipeline will be created in the AWS Console (<a href="https://us-west-2.console.aws.amazon.com/codepipeline/home?region=us-west-2#/create/Name" class="calibre7">https://us-west-2.console.aws.amazon.com/codepipeline/home?region=us-west-2#/create/Name</a> [user must be logged in to access]). Note, in the buildspec.yml, that a “fake” set of credentials is created in the docker container that CodeBuild uses. This is for the Moto library that mocks out Python  Boto calls.</p>
<div class="side-exe">
<p class="ex-caption"><a id="ch2list6" class="calibre20"></a><span class="calibre3">Listing 2.6</span> <strong class="calibre3">Click Command-line Testing of pcli</strong></p>
<p class="codelink"><a id="p041pro01" href="part0031_split_034.html#p041pro01a" class="calibre7">Click here to view code image</a></p>
<p class="hr"></p>
<p class="pre-ex">version: 0.2<br class="calibre9"/>
<br class="calibre9"/>
phases:<br class="calibre9"/>
  install:<br class="calibre9"/>
    commands:<br class="calibre9"/>
      - echo "Upgrade Pip and install packages"<br class="calibre9"/>
      - pip install --upgrade pip<br class="calibre9"/>
      - make install<br class="calibre9"/>
            # create the aws dir<br class="calibre9"/>
      - mkdir -p ~/.aws/<br class="calibre9"/>
      # create fake credential file<br class="calibre9"/>
      - echo "[default]\naws_access_key_id = \<br class="calibre9"/>
        FakeKey\naws_secret_access_key = \<br class="calibre9"/>
        FakeKey\naws_session_token = FakeKey" &gt;\<br class="calibre9"/>
        ~/.aws/credentials<br class="calibre9"/>
<br class="calibre9"/>
  build:<br class="calibre9"/>
    commands:<br class="calibre9"/>
      - echo "Run lint and test"<br class="calibre9"/>
      - make lint<br class="calibre9"/>
      - PYTHONPATH=".";make test<br class="calibre9"/>
  post_build:<br class="calibre9"/>
    commands:<br class="calibre9"/>
      - echo Build completed on `date`</p>
</div>
<p class="noindent"><span epub:type="pagebreak" id="page_42"></span>In the console for AWS CodePipeline, there are few steps to get the build working. First, in <a href="part0011.html#ch2fig1" class="calibre7">Figure 2.1</a>, the pipeline name “paws” is created.</p>
<div class="figure">
<div class="image"><a id="ch2fig1" class="calibre7"></a><img src="../images/00002.jpeg" aria-describedby="alt_02fig01" alt="A screenshot of an AWS CodePipeline for creating the name is shown." class="calibre8"/>
<aside class="hidden" id="alt_02fig01" data-AmznRemoved-M8="true" data-AmznRemoved="mobi7">
<p class="calibre21">The screen displays, “Create pipeline” on the left. The left section displays 6 steps to create CodePipeline, Step 1: Name; Step 2: Source (disabled); Step 3: Build; Step 4: Display; Step 5: Service Role; and Step 6: Review. The right section reads, Getting started with A W S CodePipeline. A textbox, Pipeline name (required) that reads: paws is shown below. At the right bottom of the section, two buttons: Cancel and Next step (selected) are shown.</p>
</aside>
</div>
<p class="fig_caption"><span class="calibre6">Figure 2.1</span> Representation of Creating a CodePipeline Name</p>
</div>
<p class="noindent">In <a href="part0011.html#ch2fig2" class="calibre7">Figure 2.2</a>, GitHub is selected as a source from which to pull, and a GitHub repository name and branch is selected. In this case, it triggers a change every time the master branch is updated.</p>
<div class="figure">
<div class="image"><a id="ch2fig2" class="calibre7"></a><img src="../images/00003.jpeg" aria-describedby="alt_02fig02" alt="A screenshot of an A W S CodePipeline for creating the source is shown." class="calibre8"/>
<aside class="hidden" id="alt_02fig02" data-AmznRemoved-M8="true" data-AmznRemoved="mobi7">
<p class="calibre21">The screen, Create pipeline displays the steps for creating the CodePipeline on the left where Step 1: Name is disabled. On the right, three sections: Source location, Connect to GitHub, and Advanced are displayed. A spin box, Source provider (required) set to “GitHub” under the Source location is shown. Two text boxes: Repository (required) that reads “noahgift/pragi-aws” and Branch (required) that reads “master” under the Connect to GitHub are shown. At the bottom right of the section, three buttons: Cancel, Previous, and Next step (selected) are shown.</p>
</aside>
</div>
<p class="fig_caption"><span class="calibre6">Figure 2.2</span> Representation of Creating a CodePipeline Source</p>
</div>
<p class="noindent"><span epub:type="pagebreak" id="page_43"></span>The next step is the build step, which has more going on than any other step (see <a href="part0011.html#ch2fig3" class="calibre7">Figure 2.3</a>).  The most important part to pay attention to is the use of a custom docker image, which shows the power of CodePipeline. Additionally, the build step is told to look for a buildspec.yml file in the root of the GitHub repository. This is the file that was created in <a href="part0011.html#ch2list5" class="calibre7">Listing 2.5</a>.</p>
<div class="figure">
<div class="image"><span epub:type="pagebreak" id="page_44"></span><a id="ch2fig3" class="calibre7"></a><img src="../images/00004.jpeg" aria-describedby="alt_02fig03" alt="A screenshot of an A W S CodePipeline for creating the Build is shown." class="calibre8"/>
<aside class="hidden" id="alt_02fig03" data-AmznRemoved-M8="true" data-AmznRemoved="mobi7">
<p class="calibre21">The screen, Create pipeline displays the steps for creating the CodePipeline on the left where Step 1 and Step 2 are disabled. On the right, four sections: Build, A W S CodeBuild, Configure your project, and Environment: How to build are displayed. A spin box, Build provider (required) set to “AWS CodeBuild” under the Build is shown. An instruction under the A W S CodeBuild reads, A W S CodeBuild is a fully managed build service that builds and tests code in the cloud. CodeBuild scales continuously. You only pay by the minute, with “Learn more” hyperlink option. The Configure your project section consists, two radio buttons: Select an existing build project and Create a new build project (selected); and two textbox, Project name (required) reads: paws and Description of 255 characters maximum reads: Build server job for paws (with Remove description option) are shown. The Environment section consists, Environment image (required) with two radio button options: Use an image managed by AWS CodeBuild and Specify a Docker image (selected); a dropdown list box, Custom image type (required) is set to “Other;” a textbox, Custom image ID reads, python:3.6.2-stretch; and Build specification with two radio button options: Use the buildspec.ymi in the source code root directory (selected) and Insert build commands are shown.</p>
</aside>
</div>
<p class="fig_caption"><span class="calibre6">Figure 2.3</span> Representation of Creating a CodePipeline Build</p>
</div>
<p class="noindent"><a href="part0011.html#ch2fig4" class="calibre7">Figure 2.4</a> is the deploy step, which is not something that will be set up. This step could deploy the project to, say, Elastic Beanstalk.</p>
<div class="figure">
<div class="image"><a id="ch2fig4" class="calibre7"></a><img src="../images/00005.jpeg" aria-describedby="alt_02fig04" alt="A screenshot of an A W S CodePipeline for creating the Deploy is shown." class="calibre8"/>
<aside class="hidden" id="alt_02fig04" data-AmznRemoved-M8="true" data-AmznRemoved="mobi7">
<p class="calibre21">The screen, Create pipeline displays the steps for creating the CodePipeline on the left where Step 1, Step 2, and Step 3 are disabled. On the right, a Deploy section shows a spin box, Deployment provider (required) is set to No Deployment and another section below reads the following instruction for the No Deployment, You are creating the pipeline without a deployment stage. You can edit your pipeline later to add one or more deployment stages. At the bottom right of the section, three buttons: Cancel, Previous, and Next step (selected) are shown.</p>
</aside>
</div>
<p class="fig_caption"><span class="calibre6">Figure 2.4</span> Representation of Creating a CodePipeline Deploy</p>
</div>
<p class="noindent"><a href="part0011.html#ch2fig5" class="calibre7">Figure 2.5</a> shows the final screen of the wizard to create a pipeline, and <a href="part0011.html#ch2fig6" class="calibre7">Figure 2.6</a> shows a successful build after a trigger from GitHub. This completes a basic setup of CodePipeline for a project, but there is much more that can be done. Lambda functions can be triggered, SNS messages can be sent, and multiple simultaneous builds can be triggered that test, say, multiple versions of your code base on different versions of Python.</p>
<div class="figure">
<div class="image"><a id="ch2fig5" class="calibre7"></a><img src="../images/00006.jpeg" aria-describedby="alt_02fig05" alt="A screenshot of an A W S CodePipeline for Review is shown." class="calibre8"/>
<aside class="hidden" id="alt_02fig05" data-AmznRemoved-M8="true" data-AmznRemoved="mobi7">
<p class="calibre21">The screen, Create pipeline displays the steps for creating the CodePipeline on the left where Step 1 to 5 are disabled. On the right, five sections: Review your pipeline, Source Stage, Build Stage, Staging Stage, and Pipeline settings. The Source Stage section consists of Source provider, Repository, and Branch details that read: GitHub, noahgift/pragai-aws, and master, respectively. The Build Stage section consists of Build provider and Project name (required) that reads, AWS CodeBuild and paws with view project details option. The Staging Stage shows Deployment provider detail that reads, No Deployment. The Pipeline settings consist of Pipeline name, Artifact location, Role name that reads: paws, s3://codepipeline-us-west-862037486407/, and AWS-CodePipeline-Service. An instruction below reads, To save this configuration with these resources, choose Create pipeline. And a question that reads, Would you like to create this pipeline? At the bottom right of the section, three buttons: Cancel, Previous, and Create pipeline (selected) are shown.</p>
</aside>
</div>
<p class="fig_caption"><span class="calibre6">Figure 2.5</span> Representation of a CodePipeline Review</p>
</div>
<div class="figure">
<div class="image"><span epub:type="pagebreak" id="page_45"></span><a id="ch2fig6" class="calibre7"></a><img src="../images/00007.jpeg" aria-describedby="alt_02fig06" alt="A screenshot of a successful CodePipeline Build is shown." class="calibre8"/>
<aside class="hidden" id="alt_02fig06" data-AmznRemoved-M8="true" data-AmznRemoved="mobi7">
<p class="calibre21">The left section of the screen displays, AWS CodeBuild with the options: Build projects and Build history (selected). The right section displays, paws:8fed00e0-da6d-4254-9539-6a7b0b11bf6d (succeeded). The Build section below shows the following fields: Build A R N, Build project, Source provider, Repository, Start time, End time, Status, and Initiator. A Build details section below consists of Phase details and Build logs. The Phase details show a table with the following column headers: Name, Status, Duration, and Completed. The build logs read, Showing the last 20 lines of build log below along with “View entire log” option.</p>
</aside>
</div>
<p class="fig_caption"><span class="calibre6">Figure 2.6</span> Representation of a Successful CodePipeline Build</p>
</div>
<h3 id="ch02lev6" class="calibre12">Basic Docker Setup for Data Science</h3>
<p class="noindent">In dealing with students new to data science, there is a constant stream of questions about the same topic: their environment doesn’t work anymore. This is a huge problem that is slowly getting much better due to tools like Docker. At least for Mac users, there are similarities in the Mac Unix environment and a production Linux environment, but Windows is a completely foreign world. This is why Docker on Windows is such a powerful tool for data scientists who use that platform.</p>
<p class="noindent">To install Docker on OS X, Linux, or Windows, you can refer to the instructions on <a href="https://www.docker.com/" class="calibre7">https://www.docker.com/</a>. A good place to experiment with a default data science stack is to use the jupyter/datascience-notebook (<a href="https://hub.docker.com/r/jupyter/datascience-notebook/" class="calibre7">https://hub.docker.com/r/jupyter/datascience-notebook/</a>). One you have Docker installed and you do a “docker pull,” to launch the notebook it is a one-line command.</p>
<p class="codelink"><a id="p045pro01" href="part0031_split_035.html#p045pro01a" class="calibre7">Click here to view code image</a></p>
<p class="pre">docker run -it --rm -p 8888:8888 jupyter/datascience-notebook</p>
<p class="noindent">To run batch jobs with AWS Batch, a production team could develop against a dockerfile on the team’s laptop, check it into source code, then register it against the AWS private Docker registry. Then when batch jobs are run, there would be a guarantee they are executed in exactly the same way as they were on the team’s laptop. This is absolutely the future, and pragmatic-minded teams would be advised to get up to speed with Docker. Not only does it save a ton of time in local development, but for many real-world problems with running jobs, Docker isn’t just optional, it is required.</p>
<h3 id="ch02lev7" class="calibre12"><span epub:type="pagebreak" id="page_46" class="calibre2"></span>Other Build Servers: Jenkins, CircleCI, and Travis</h3>
<p class="noindent">Although CodePipeline was covered in this chapter, it is an AWS-specific product. Other services that are very good as well include Jenkins (<a href="https://jenkins.io/" class="calibre7">https://jenkins.io/</a>), CircleCI (<a href="https://circleci.com/" class="calibre7">https://circleci.com/</a>), Codeship (<a href="https://codeship.com/" class="calibre7">https://codeship.com/</a>), and Travis (<a href="https://travis-ci.org/" class="calibre7">https://travis-ci.org/</a>). They all have strengths and weaknesses, but in general they are also Docker-based build systems. This is yet another reason to get a strong foundation around Docker.</p>
<p class="noindent">For further inspiration, you can look at a sample project I created that builds on CircleCI:  <a href="https://github.com/noahgift/myrepo" class="calibre7">https://github.com/noahgift/myrepo</a>. There is also a video you can watch from a workshop I taught where I go over the details step by step.</p>
<h3 id="ch02lev8" class="calibre12">Summary</h3>
<p class="noindent">This chapter covered the fundamentals of DevOps as it relates to ML. A sample continuous-delivery pipeline and project structure was created that could be used as the building blocks of an ML pipeline. It’s easy to get months down the road on a project and realize that a lack of fundamental infrastructure is going to eventually cause an existential threat to the project’s long-term viability.</p>
<p class="noindent">Finally, Docker was covered in detail because, frankly, it is the future. All data teams need to know about it. And for truly large problems, like building a large-scale production AI system, it will absolutely part of the deployed solution.</p>
</body></html>
